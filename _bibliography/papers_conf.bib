---
---

@string{aps = {American Physical Society,}}

@inproceedings{vanderValk:2023,
  abbr =    {MICCAI},
  bibtex_show = {true},
  author =    {van der Valk, Viktor and Atsma, Douwe and Scherptong, Roderick and Staring, Marius},
  title =     {Joint optimization of a {\beta}-VAE for ECG task-specific feature extraction},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  address =   {Vancouver, Canada},
  series =    {Lecture Notes in Computer Science},
  volume =    {14221},
  pages =     {554 -- 563},
  month =     {October},
  year =      {2023},
  pdf =       {2023_c_MICCAI.pdf},
  html =      {https://doi.org/10.1007/978-3-031-43895-0_52},
  arxiv =     {2304.06476},
  abstract={Electrocardiography is the most common method to investigate the condition of the heart through the observation of cardiac rhythm and electrical activity, for both diagnosis and monitoring purposes. Analysis of electrocardiograms (ECGs) is commonly performed through the investigation of specific patterns, which are visually recognizable by trained physicians and are known to reflect cardiac (dis)function. In this work we study the use of Î²-variational autoencoders (VAEs) as an explainable feature extractor, and improve on its predictive capacities by jointly optimizing signal reconstruction and cardiac function prediction. The extracted features are then used for cardiac function prediction using logistic regression. The method is trained and tested on data from 7255 patients, who were treated for acute coronary syndrome at the Leiden University Medical Center between 2010 and 2021. The results show that our method significantly improved prediction and explainability compared to a vanilla Î²-VAE, while still yielding similar reconstruction performance.}
}

@inproceedings{Ntatsis:2023,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Ntatsis, Konstantinos and Dekker, Niels and van der Valk, Viktor and Birdsong, Tom and Zukic, Dzenan and Klein, Stefan and Staring, Marius and McCormick, Matthew},
title 	= {itk-elastix: Medical image registration in Python},
booktitle 	= {Proceedings of the 22nd Python in Science Conference},
editor 	= {Agarwal, Meghann and Calloway, Chris and Niederhut, Dillon},
pages 	= {101 - 105},
month 	= {July},
year 	= {2023},
  pdf =       {2023_c_scipy.pdf},
  html =      {http://dx.doi.org/10.25080/gerudo-f2bc6f59-00d},
  arxiv =     {},
  code =      {https://github.com/InsightSoftwareConsortium/ITKElastix},
  abstract =  {Image registration plays a vital role in understanding changes that occur in 2D and 3D scientific imaging datasets. Registration involves finding a spatial transformation that aligns one image to another by optimizing relevant image similarity metrics. In this paper, we introduce itk-elastix, a user-friendly Python wrapping of the mature elastix registration toolbox. The open-source tool supports rigid, affine, and B-spline deformable registration, making it versatile for various imaging datasets. By utilizing the modular design of itk-elastix, users can efficiently configure and compare different registration methods, and embed these in image analysis workflows.},
}

@inproceedings{Chen:2023,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Chen, Yunjie and Staring, Marius and Wolterink, Jelmer M. and Tao, Qian},
title 	= {Local implicit neural representations for multi-sequence MRI translation},
booktitle 	= {IEEE International Symposium on Biomedical Imaging (ISBI)},
address 	= {Cartagena de Indias, Colombia},
month 	= {April},
year 	= {2023},
  pdf =       {2023_c_ISBI.pdf},
  html =      {https://doi.org/10.1109/ISBI53787.2023.10230409},
  arxiv =     {https://arxiv.org/abs/2302.01031},
  code =      {},
  abstract =  {In radiological practice, multi-sequence MRI is routinely acquired to characterize anatomy and tissue. However, due to the heterogeneity of imaging protocols and contra-indications to contrast agents, some MRI sequences, e.g. contrast-enhanced T1-weighted image (T1ce), may not be acquired. This creates difficulties for large-scale clinical studies for which heterogeneous datasets are aggregated. Modern deep learning techniques have demonstrated the capability of synthesizing missing sequences from existing sequences, through learning from an extensive multi-sequence MRI dataset. In this paper, we propose a novel MR image translation solution based on <em>local implicit neural representations</em>. We split the available MRI sequences into local patches and assign to each patch a local multi-layer perceptron (MLP) that represents a patch in the T1ce. The parameters of these local MLPs are generated by a hypernetwork based on image features. Experimental results and ablation studies on the BraTS challenge dataset showed that the local MLPs are critical for recovering fine image and tumor details, as they allow for local specialization that is highly important for accurate image translation. Compared to a classical pix2pix model, the proposed method demonstrated visual improvement and significantly improved quantitative scores (MSE 0.86 {\times} 10<sup>-3</sup> vs. 1.02 {\times} 10<sup>-3</sup> and SSIM 94.9 vs 94.3).},
}

@inproceedings{Tan:2023,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Tan, Yicong and Mody, Prerak and van der Valk, Viktor and Staring, Marius and van Gemert, Jan},
title 	= {Analyzing Components of a Transformer under Different Data Scales in 3D Prostate CT Segmentation},
booktitle 	= {SPIE Medical Imaging: Computer-Aided Diagnosis},
editor 	= {Colliot, Olivier and I{\v{s}}gum, Ivana},
address 	= {San Diego, CA, USA},
series 	= {Proceedings of SPIE},
volume 	= {12464},
pages 	= {1246408},
month 	= {February},
year 	= {2023},
  pdf =       {2023_c_SPIEMI.pdf},
  html =      {http://dx.doi.org/10.1117/12.2651572},
  arxiv =     {},
  code =      {https://github.com/prerakmody/window-transformer-prostate-segmentation},
  abstract =  {Literature on medical imaging segmentation claims that hybrid UNet models containing both Transformer and convolutional blocks perform better than purely convolutional UNet models. This recently touted success of Transformers warrants an investigation into which of its components contribute to its performance. Also, previous work has a limitation of analysis only at fixed data scales as well as unfair comparisons with others models where parameter counts are not equivalent. This work investigates the performance of the window-Based Transformer for prostate CT Organ-at-Risk (OAR) segmentation at different data scales in context of replacing its various components. To compare with literature, the first experiment replaces the window-based Transformer block with convolution. Results show that the convolution prevails as the data scale increases. In the second experiment, to reduce complexity, the self-attention mechanism is replaced with an equivalent albeit simpler spatial mixing operation i.e. max-pooling. We observe improved performance for max-pooling in smaller data scales, indicating that the window-based Transformer may not be the best choice in both small and larger data scales. Finally, since convolution has an inherent local inductive bias of positional information, we conduct a third experiment to imbibe such a property to the Transformer by exploring two kinds of positional encodings. The results show that there are insignificant improvements after adding positional encoding, indicating the Transformers deficiency in capturing positional information given our data scales. We hope that our approach can serve as a framework for others evaluating the utility of Transformers for their tasks. Code is available via <a href="https://github.com/prerakmody/window-transformer-prostate-segmentation">GitHub</a>.},
}

@inproceedings{Mody:2022,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Mody, Prerak P. and Chaves-de-Plaza, Nicolas F. and Hildebrandt, Klaus and Staring, Marius},
title 	= {Improving Error Detection in Deep Learning based Radiotherapy Autocontours using Bayesian Uncertainty},
booktitle 	= {Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, MICCAI workshop},
address 	= {Singapore},
series 	= {Lecture Notes in Computer Science},
volume 	= {13563},
pages 	= {70 - 79},
month 	= {September},
year 	= {2022},
  pdf =       {2022_c_MICCAI-UNSURE.pdf},
  html =      {https://doi.org/10.1007/978-3-031-16749-2_7},
  arxiv =     {},
  code =      {},
  abstract =  {Bayesian Neural Nets (BNN) are increasingly used for robust organ auto-contouring. Uncertainty heatmaps extracted from BNNs have been shown to correspond to inaccurate regions. To help speed up the mandatory quality assessment (QA) of contours in radiotherapy, these heatmaps could be used as stimuli to direct visual attention of clinicians to potential inaccuracies. In practice, this is non-trivial to achieve since many accurate regions also exhibit uncertainty. To influence the output uncertainty of a BNN, we propose a modified accuracy-versus-uncertainty (AvU) metric as an additional objective during model training that penalizes both accurate regions exhibiting uncertainty as well as inaccurate regions exhibiting certainty. For evaluation, we use an uncertainty-ROC curve that can help differentiate between Bayesian models by comparing the probability of uncertainty in inaccurate versus accurate regions. We train and evaluate a FlipOut BNN model on the MICCAI2015 Head and Neck Segmentation challenge dataset and on the DeepMind-TCIA dataset, and observed an increase in the AUC of uncertainty-ROC curves by 5.6% and 5.9%, respectively, when using the AvU objective. The AvU objective primarily reduced false positives regions (uncertain and accurate), drawing less visual attention to these regions, thereby potentially improving the speed of error detection.},
}

@inproceedings{Chaves-de-Plaza:2022,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Chaves-de-Plaza, Nicolas and Mody, Prerak P. and Hildebrandt, Klaus and de Ridder, Huib and Staring, Marius and van Egmond, René},
title 	= {Towards Fast and Robust AI-Infused Human-Centered Contouring Workflows For Adaptive Proton Therapy in the Head and Neck},
booktitle 	= {European Chapter of the Human Factors and Ergonomics Society},
address 	= {Turin, Italy},
month 	= {April},
year 	= {2022},
  pdf =       {2022_c_HFES.pdf},
  html =      {},
  arxiv =     {https://arxiv.org/abs/2208.04675},
  code =      {},
  abstract =  {Delineation of tumors and organs-at-risk permits detecting and correcting changes in the patients' anatomy throughout the treatment, making it a core step of adaptive proton therapy (APT). Although AI-based auto-contouring technologies have sped up this process, the time needed to perform the quality assessment (QA) of the generated contours remains a bottleneck, taking clinicians between several minutes up to an hour to complete. This paper introduces a fast contouring workflow suitable for time-critical APT, enabling detection of anatomical changes in shorter time frames and with a lower demand of clinical resources. The proposed human-centered AI-infused workflow follows two principles uncovered after reviewing the APT literature and conducting several interviews and an observational study in two radiotherapy centers in the Netherlands. First, enable targeted inspection of the generated contours by leveraging AI uncertainty and clinically-relevant features such as the proximity of the organs-at-risk to the tumor. Second, minimize the number of interactions needed to edit faulty delineations with redundancy-aware editing tools that provide the user a sense of predictability and control. We use a proof of concept that we validated with clinicians to demonstrate how current and upcoming AI capabilities support the workflow and how it would fit into clinical practice.},
}

@inproceedings{Jia:2022,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Jia, Jingnan and Staring, Marius and Hernández Girón, Irene and Kroft, Lucia J.M. and Schouffoer, Anne A. and Stoel, Berend C.},
title 	= {Prediction of Lung CT Scores of Systemic Sclerosis by Cascaded Regression Neural Networks},
booktitle 	= {SPIE Medical Imaging: Computer-Aided Diagnosis},
editor 	= {Colliot, Olivier and Isgum, Ivana},
address 	= {San Diego, CA, USA},
series 	= {Proceedings of SPIE},
volume 	= {12033},
pages 	= {1203338},
month 	= {February},
year 	= {2022},
  pdf =       {2022_c_SPIEMIa.pdf},
  html =      {https://doi.org/10.1117/12.2602737},
  arxiv =     {},
  code =      {},
  abstract =  {Visually scoring lung involvement in systemic sclerosis from CT scans plays an important role in monitoring progression, but  its  labor  intensiveness  hinders practical  application.  We  proposed,  therefore,  an  automatic  scoring  framework  that consists of two cascaded deep regression neural networks. The first (3D) network aims to predict the craniocaudal position of five anatomically defined scoring levels on the 3D CT scans. The second (2D) network receives the resulting 2D axial slices  and predicts  the  scores.  We  used  227 3D CT scans to train and validate  the  first network, and the  resulting  1135 axial slices were used in the second network. Two experts scored independently a subset of data to obtain intra- and inter-observer variabilities and the ground truth for all data was obtained in consensus. To alleviate the unbalance in training labels  in  the  second  network,  we  introduced  a  sampling  technique  and  to  increase  the  diversity  of  the  training  samples synthetic data was generated, mimicking ground glass and reticulation patterns. The 4-fold cross validation showed that our proposed network achieved an average MAE of 5.90, 4.66 and 4.49, weighted kappa of 0.66, 0.58 and 0.65 for total score (TOT), ground glass (GG) and reticular pattern (RET), respectively. Our network performed slightly worse than the best experts on TOT and GG prediction but it has competitive performance on RET prediction and has the potential to be an objective alternative for the visual scoring of SSc in CT thorax studies.},
}

@inproceedings{Mody:2022,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Mody, Prerak P and Chaves-de-Plaza, Nicolas and Hildebrandt, Klaus and van Egmond, René and Villanova, Anna and Staring, Marius},
title 	= {Comparing Bayesian Models for Organ Contouring in Head and Neck Radiotherapy},
booktitle 	= {SPIE Medical Imaging: Image Processing},
editor 	= {Colliot, Olivier and Isgum, Ivana},
address 	= {San Diego, CA, USA},
series 	= {Proceedings of SPIE},
volume 	= {12032},
pages 	= {120320F},
month 	= {February},
year 	= {2022},
  pdf =       {2022_c_SPIEMIb.pdf},
  html =      {https://doi.org/10.1117/12.2611083},
  arxiv =     {},
  code =      {},
  abstract =  {Deep learning models for organ contouring in radiotherapy are poised for clinical usage, but currently, there exist few tools for automated quality assessment (QA) of the predicted contours. Bayesian models and their associated uncertainty, can potentially automate the process of detecting inaccurate predictions. We investigate two Bayesian models for auto-contouring, DropOut and FlipOut, using a quantitative measure - expected calibration error (ECE) and a qualitative measure - region-based accuracy-vs-uncertainty (R-AvU) graphs. It is well understood that a model should have low ECE to be considered trustworthy. However, in a QA context, a model should also have high uncertainty in inaccurate regions and low uncertainty in accurate regions. Such behaviour could direct visual attention of expert users to potentially inaccurate regions, leading to a speed-up in the QA process. Using R-AvU graphs, we qualitatively compare the behaviour of different models in accurate and inaccurate regions. Experiments are conducted on the MICCAI2015 Head and Neck Segmentation Challenge and on the DeepMindTCIA CT dataset using three models: DropOut-DICE, Dropout-CE (Cross Entropy) and FlipOut-CE. Quantitative results show that DropOut-DICE has the highest ECE, while Dropout-CE and FlipOut-CE have the lowest ECE. To better understand the difference between DropOut-CE and FlipOut-CE, we use the R-AvU graph which shows that FlipOut-CE has better uncertainty coverage in inaccurate regions than DropOut-CE. Such a combination of quantitative and qualitative metrics explores a new approach that helps to select which model can be deployed as a QA tool in clinical settings.},
}

@inproceedings{Li:2022,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Li, Yichao and Elmahdy, Mohamed S. and Lew, Michael S. and Staring, Marius},
title 	= {Transformation-Consistent Semi-Supervised Learning for Prostate CT Radiotherapy},
booktitle 	= {SPIE Medical Imaging: Computer-Aided Diagnosis},
editor 	= {Colliot, Olivier and Isgum, Ivana},
address 	= {San Diego, CA, USA},
series 	= {Proceedings of SPIE},
volume 	= {12033},
pages 	= {120333O},
month 	= {February},
year 	= {2022},
  pdf =       {2022_c_SPIEMIc.pdf},
  html =      {https://doi.org/10.1117/12.2604968},
  arxiv =     {},
  code =      {},
  abstract =  {Deep supervised models often require a large amount of labelled data, which is difficult to obtain in the medical domain. Therefore, semi-supervised learning (SSL) has been an active area of research due to its promise to minimize training costs by leveraging unlabelled data. Previous research have shown that SSL is especially effective in low labelled data regimes, we show that outperformance can be extended to high data regimes by applying Stochastic Weight Averaging (SWA), which incurs zero additional training cost. Our model was trained on a prostate CT dataset and achieved improvements of 0.12 mm, 0.14 mm, 0.32 mm, and 0.14 mm for the prostate, seminal vesicles, rectum, and bladder respectively, in terms of median test set mean surface distance (MSD) compared to the supervised baseline in our high data regime.},
}

@inproceedings{Johnson:2021,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Johnson, Patricia M. and Jeong, Geunu and Hammernik, Kerstin and Schlemper, Jo and Qin, Chen and Duan, Jinming and Rueckert, Daniel and Lee, Jingu and Pezzotti, Nicola and De Weerdt, Elwin and Yousefi, Sahar and Elmahdy, Mohamed S. and Van Gemert, Jeroen Hendrikus Franciscus and Schuelke, Chistophe and Doneva, Mariya and Nielsen, Tim and Kastryulin, Sergey and Lelieveldt, Boudewijn P. F. and Van Osch, Matthias J. P. and Staring, Marius and Chen, Eric Z. and Wang, Puyang and Chen, Xiao and Chen, Terrence and Patel, Vishal M. and Sun, Shanhui and Shin, Hyungseob and Jun, Yohan and Eo, Taejoon and Kim, Sewon and Kim, Taeseong and Hwang, Dosik and Putzky, Patrick and Karkalousos, Dimitrios and Teuwen, Jonas and Miriakov, Nikita and Bakker, Bart and Caan, Matthan and Welling, Max and Muckley, Matthew J. and Knoll, Florian},
title 	= {Evaluation of the Robustness of Learned MR Image Reconstruction to Systematic Deviations Between Training and Test Data for the Models from the fastMRI Challenge},
booktitle 	= {Machine Learning for Medical Image Reconstruction, MICCAI workshop},
editor 	= {Haq, Nandinee F. and Johnson, Patricia and Maier, Andreas and Würfl, Tobias and Yoo, Jaejun},
address 	= {Strasbourg, France},
series 	= {Lecture Notes in Computer Science},
volume 	= {12964},
pages 	= {25 - 34},
month 	= {October},
year 	= {2021},
  pdf =       {2021_c_MICCAI-MLMIR.pdf},
  html =      {https://doi.org/10.1007/978-3-030-88552-6_3},
  arxiv =     {},
  code =      {},
  abstract =  {The 2019 fastMRI challenge was an open challenge designed to advance research in the field of machine learning for MR image reconstruction. The goal for the participants was to reconstruct undersampled MRI k-space data. The original challenge left an open question as to how well the reconstruction methods will perform in the setting where there is a systematic difference between training and test data. In this work we tested the generalization performance of the submissions with respect to various perturbations, and despite differences in model architecture and training, all of the methods perform very similarly.},
}

@inproceedings{Jia:2021,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Jia, Jingnan and Zhai, Zhiwei and Bakker, M. Els and Hern{\'a}ndez Gir{\'o}n, I. and Staring, Marius and Stoel, Berend C.},
title 	= {Multi-Task Semi-Supervised Learning for Pulmonary Lobe Segmentation},
booktitle 	= {IEEE International Symposium on Biomedical Imaging (ISBI)},
address 	= {Nice, France},
pages 	= {1329 - 1332},
month 	= {April},
year 	= {2021},
  pdf =       {2021_c_ISBI.pdf},
  html =      {https://doi.org/10.1109/ISBI48211.2021.9433985},
  arxiv =     {},
  code =      {},
  abstract =  {Pulmonary lobe segmentation is an important preprocessing task for the analysis of lung diseases. Traditional methods relying on fissure detection or other anatomical features, such as the distribution of pulmonary vessels and airways, could provide reasonably accurate lobe segmentations. Deep learning based methods can outperform these traditional approaches, but require large datasets. Deep multi-task learning is expected to utilize labels of multiple different structures. However, commonly such labels are distributed over multiple datasets. In this paper, we proposed a multi-task semi-supervised model that can leverage information of multiple structures from unannotated datasets and datasets annotated with different structures. A focused alternating training strategy is presented to balance the different tasks. We evaluated the trained model on an external independent CT dataset. The results show that our model significantly outperforms single-task alternatives, improving the mean surface distance from 7.174 mm to 4.196 mm. We also demonstrated that our approach is successful for different network architectures as backbones.},
}

@inproceedings{Beljaards:2020,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Beljaards, Laurens and Elmahdy, Mohamed S. and Verbeek, Fons and Staring, Marius},
title 	= {A Cross-Stitch Architecture for Joint Registration and Segmentation in Adaptive Radiotherapy},
booktitle 	= {Medical Imaging with Deep Learning},
editor 	= {Pal, Christopher and Descoteaux, Maxime},
address 	= {Montreal, Canada},
series 	= {Proceedings of Machine Learning Research},
volume 	= {121},
pages 	= {62 - 74},
month 	= {July},
year 	= {2020},
  pdf =       {2020_c_MIDL.pdf},
  html =      {http://proceedings.mlr.press/v121/beljaards20a.html},
  arxiv =     {https://arxiv.org/abs/2004.08122},
  code =      {},
  abstract =  {Recently, joint registration and segmentation has been formulated in a deep learning setting, by the definition of joint loss functions. In this work, we investigate joining these tasks at the architectural level. We propose a registration network that integrates segmentation propagation between images, and a segmentation network to predict the segmentation directly. These networks are connected into a single joint architecture via so-called cross-stitch units, allowing information to be exchanged between the tasks in a learnable manner. The proposed method is evaluated in the context of adaptive image-guided radiotherapy, using daily prostate CT imaging. Two datasets from different institutes and manufacturers were involved in the study. The first dataset was used for training (12 patients) and validation (6 patients), while the second dataset was used as an independent test set (14 patients). In terms of mean surface distance, our approach achieved 1.06 {\pm} 0.3 mm, 0.91 {\pm} 0.4 mm, 1.27 {\pm} 0.4 mm, and 1.76 {\pm} 0.8 mm on the validation set and 1.82 {\pm} 2.4 mm, 2.45 {\pm} 2.4 mm, 2.45 {\pm} 5.0 mm, and 2.57 {\pm} 2.3 mm on the test set for the prostate, bladder, seminal vesicles, and rectum, respectively. The proposed multi-task network outperformed single-task networks, as well as a network only joined through the loss function, thus demonstrating the capability to leverage the individual strengths of the segmentation and registration tasks. The obtained performance as well as the inference speed make this a promising candidate for daily re-contouring in adaptive radiotherapy, potentially reducing treatment-related side effects and improving quality-of-life after treatment.},
}

@inproceedings{Elmahdy:2020,
  abbr =    {},
  bibtex_show = {true},
  author 	= {Elmahdy, Mohamed S. and Ahuja, Tanuj and van der Heide, Uulke A. and Staring, Marius},
title 	= {Patient-Specific Finetuning of Deep Learning Models for Adaptive Radiotherapy in Prostate CT},
booktitle 	= {IEEE International Symposium on Biomedical Imaging (ISBI)},
address 	= {Iowa City, Iowa, USA},
pages 	= {577 - 580},
month 	= {April},
year 	= {2020},
  pdf =       {2020_c_ISBI.pdf},
  html =      {https://doi.org/10.1109/ISBI45749.2020.9098702},
  arxiv =     {},
  code =      {},
  abstract =  {Contouring of the target volume and Organs-At-Risk (OARs) is a crucial step in radiotherapy treatment planning. In an adaptive radiotherapy setting, updated contours need to be generated based on daily imaging. In this work, we leverage personalized anatomical knowledge accumulated over the treatment sessions, to improve the segmentation accuracy of a pre-trained Convolution Neural Network (CNN), for a specific patient. We investigate a transfer learning approach, finetuning the baseline CNN model to a specific patient, based on imaging acquired in earlier treatment fractions. The baseline CNN model is trained on a prostate CT dataset from one hospital of 379 patients. This model is then fine-tuned and tested on an independent dataset of another hospital of 18 patients, each having 7 to 10 daily CT scans. For the prostate, seminal vesicles, bladder and rectum, the model fine-tuned on each specific patient achieved a Mean Surface Distance (MSD) of 1:64 {\pm} 0:43 mm, 2:38 {\pm} 2:76 mm, 2:30 {\pm} 0:96 mm, and 1:24 {\pm} 0:89 mm, respectively, which was significantly better than the baseline model. The proposed personalized model adaptation is therefore very promising for clinical implementation in the context of adaptive radiotherapy of prostate cancer.},
}

@inproceedings{deVos:2020,
  abbr =    {},
  bibtex_show = {true},
  author 	= {de Vos, Bob D. and van der Velden, Bas H. M. and Sander, Jörg and Gilhuijs, Kenneth G.A. and Staring, Marius and I{\v{s}}gum, Ivana},
title 	= {Mutual information for unsupervised deep learning image registration},
booktitle 	= {SPIE Medical Imaging: Image Processing},
editor 	= {I{\v{s}}gum, Ivana and Landman, Bennett A.},
address 	= {Houston, Texas, USA},
series 	= {Proceedings of SPIE},
volume 	= {11313},
pages 	= {11313OR},
month 	= {February},
year 	= {2020},
  pdf =       {2020_c_SPIEMI.pdf},
  html =      {https://doi.org/10.1117/12.2549729},
  arxiv =     {},
  code =      {},
  abstract =  {Current unsupervised deep learning-based image registration methods are trained with mean squares or normalized cross correlation as a similarity metric. These metrics are suitable for registration of images where a linear relation between image intensities exists. When such a relation is absent knowledge from conventional image registration literature suggests the use of mutual information. In this work we investigate whether mutual information can be used as a loss for unsupervised deep learning image registration by evaluating it on two datasets: breast dynamic contrast-enhanced MR and cardiac MR images. The results show that training with mutual information as a loss gives on par performance compared with conventional image registration in contrast enhanced images, and the results show that it is generally applicable since it has on par performance compared with normalized cross correlation in single-modality registration.},
}

@inproceedings{XXXX,
  abbr =    {},
  bibtex_show = {true},
  pdf =       {},
  html =      {},
  arxiv =     {},
  code =      {},
  abstract =  {},
}

