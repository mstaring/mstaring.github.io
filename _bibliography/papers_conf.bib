---
---

@string{aps = {American Physical Society,}}

@inproceedings{Zheng:2025b,
  abbr =    {},
  bibtex_show = {true},
  author =    {Zheng, Yuxi and Feng, Jianhui and Li, Tianran and Staring, Marius and Qiao, Yuchuan},
  title =     {SHMoAReg: Spark Deformable Image Registration via Spatial Heterogeneous Mixture of Experts and Attention Heads},
  booktitle = {IEEE International Conference on Bioinformatics and Biomedicine},
  address =   {Wuhan, China},
  series =    {},
  volume =    {},
  pages =     {},
  month =     {December},
  year =      {2025},
  pdf =       {},
  html =      {},
  arxiv =     {2509.20073},
  code =      {},
  abstract =  {Encoder-Decoder architectures are widely used in deep learning-based Deformable Image Registration (DIR), where the encoder extracts multi-scale features and the decoder predicts deformation fields by recovering spatial locations. However, current methods lack specialized extraction of features (that are useful for registration) and predict deformation jointly and homogeneously in all three directions. In this paper, we propose a novel expert-guided DIR network with Mixture of Experts (MoE) mechanism applied in both encoder and decoder, named SHMoAReg. Specifically, we incorporate Mixture of Attention heads (MoA) into encoder layers, while Spatial Heterogeneous Mixture of Experts (SHMoE) into the decoder layers. The MoA enhances the specialization of feature extraction by dynamically selecting the optimal combination of attention heads for each image token. Meanwhile, the SHMoE predicts deformation fields heterogeneously in three directions for each voxel using experts with varying kernel sizes. Extensive experiments conducted on two publicly available datasets show consistent improvements over various methods, with a notable increase from 60.58% to 65.58% in Dice score for the abdominal CT dataset. Furthermore, SHMoAReg enhances model interpretability by differentiating experts' utilities across/within different resolution layers. To the best of our knowledge, we are the first to introduce MoE mechanism into DIR tasks. The code will be released soon.},
}

@inproceedings{Lyu:2025b,
  abbr =    {},
  bibtex_show = {true},
  author =    {Lyu, Donghang and Staring, Marius and Lamb, Hildo and Doneva, Mariya},
  title =     {CRUNet-MR-Univ: A Foundation Model for Diverse Cardiac MRI Reconstruction},
  booktitle = {CMRxRecon2025 challenge at STACOM, MICCAI},
  address =   {Daejeon, South Korea},
  series =    {Lecture Notes in Computer Science},
  volume =    {},
  pages =     {},
  month =     {September},
  year =      {2025},
  pdf =       {2025_c_STACOMb.pdf},
  html =      {},
  arxiv =     {},
  code =      {},
  abstract =  {In recent years, deep learning has attracted increasing attention in the field of Cardiac MRI (CMR) reconstruction due to its superior performance over traditional methods, particularly in handling higher acceleration factors, highlighting its potential for real-world clinical applications. However, current deep learning methods remain limited in generalizability. CMR scans exhibit wide variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most existing models are designed to handle only a single or narrow subset of these variations, leading to performance degradation when faced with distribution shifts. Therefore, it is beneficial to develop a unified model capable of generalizing across diverse CMR scenarios. To this end, we propose CRUNet-MR-Univ, a foundation model that leverages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans. Our approach consistently outperforms baseline methods across a wide range of settings, highlighting its effectiveness and promise.},
}

@inproceedings{Lyu:2025a,
  abbr =    {},
  bibtex_show = {true},
  author =    {Lyu, Donghang and Staring, Marius and Doneva, Mariya and Lamb, Hildo and Pezzotti, Nicola},
  title =     {KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction},
  booktitle = {Statistical Atlases and Computational Modeling of the Heart (STACOM) at MICCAI},
  address =   {Daejeon, South Korea},
  series =    {Lecture Notes in Computer Science},
  volume =    {},
  pages =     {},
  month =     {September},
  year =      {2025},
  pdf =       {2025_c_STACOMa.pdf},
  html =      {},
  arxiv =     {2508.12147},
  code =      {},
  abstract =  {Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for assessing cardiac structure, function, and blood flow. Cine MRI extends this by capturing heart motion, providing detailed insights into cardiac mechanics. To reduce scan time and breath-hold discomfort, fast acquisition techniques have been utilized at the cost of lowering image quality. Recently, Implicit Neural Representation (INR) methods have shown promise in unsupervised reconstruction by learning coordinate-to-value mappings from undersampled data, enabling high quality image recovery. However, current existing INR methods primarily focus on using coordinate-based positional embeddings to learn the mapping, while overlooking the feature representations of the target point and its neighboring context. In this work, we propose KP-INR, a dual-branch INR method operating in k-space for cardiac cine MRI reconstruction: one branch processes the positional embedding of k-space coordinates, while the other learns from local multi-scale k-space feature representations at those coordinates. By enabling cross-branch interaction and approximating the target k-space values from both branches, KP-INR can achieve strong performance on challenging Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its improved performance over baseline models and highlights its potential in this field.},
}

@inproceedings{Gao:2025,
  abbr =    {},
  bibtex_show = {true},
  author =    {Gao, Ruochen and Lyu, Donghang and Staring, Marius},
  title =     {Swin-LiteMedSAM: A Lightweight Box-Based Segment Anything Model for Large-Scale Medical Image Datasets},
  booktitle = {Medical Image Segmentation Foundation Models. CVPR 2024 Challenge: Segment Anything in Medical Images on Laptop},
  address =   {},
  series =    {Lecture Notes in Computer Science},
  volume =    {15458},
  pages =     {70 -- 82},
  month =     {},
  year =      {2025},
  pdf =       {2025_c_CVPR.pdf},
  html =      {https://doi.org/10.1007/978-3-031-81854-7_5},
  arxiv =     {2409.07172},
  code =      {},
  abstract =  {Medical imaging is essential for the diagnosis and treatment of diseases, with medical image segmentation as a subtask receiving high attention. However, automatic medical image segmentation models are typically task-specific and struggle to handle multiple scenarios, such as different imaging modalities and regions of interest. With the introduction of the Segment Anything Model (SAM), training a universal model for various clinical scenarios has become feasible. Recently, several Medical SAM (MedSAM) methods have been proposed, but these models often rely on heavy image encoders to achieve high performance, which may not be practical for real-world applications due to their high computational demands and slow inference speed. To address this issue, a lightweight version of the MedSAM (LiteMedSAM) can provide a viable solution, achieving high performance while requiring fewer resources and less time. In this work, we introduce Swin-LiteMedSAM, a new variant of LiteMedSAM. This model integrates the tiny Swin Transformer as the image encoder, incorporates multiple types of prompts, including box-based points and scribble generated from a given bounding box, and establishes skip connections between the image encoder and the mask decoder. In the <i>Segment Anything in Medical Images on Laptop</i> challenge (CVPR 2024), our approach strikes a good balance between segmentation performance and speed, demonstrating significantly improved overall results across multiple modalities compared to the LiteMedSAM baseline provided by the challenge organizers. Our proposed model achieved a DSC score of 0.8678 and an NSD score of 0.8844 on the validation set. On the final test set, it attained a DSC score of 0.8193 and an NSD score of 0.8461, securing fourth place in the challenge. The code and trained model are available at https://github.com/RuochenGao/Swin_LiteMedSAM.},
}

@inproceedings{Lyu:2024,
  abbr =    {},
  bibtex_show = {true},
  author =    {Lyu, Donghang and Rao, Chinmay S. and Staring, Marius and van Osch, Matthias J.P. and Doneva, Mariya and Lamb, Hildo and Pezzotti, Nicola},
  title =     {UPCMR: A Universal Prompt-guided Model for Random Sampling Cardiac MRI Reconstruction},
  booktitle = {Statistical Atlases and Computational Modeling of the Heart (STACOM)},
  address =   {Marrakech, Morocco},
  series =    {Lecture Notes in Computer Science},
  volume =    {15448},
  pages =     {453 -- 463},
  month =     {October},
  year =      {2024},
  pdf =       {2024_c_STACOM.pdf},
  html =      {https://doi.org/10.1007/978-3-031-87756-8_44},
  arxiv =     {},
  code =      {},
  abstract =  {Cardiac magnetic resonance imaging (CMR) is a crucial tool for diagnosing and treating cardiac diseases. However, the lengthy scanning time remains a significant drawback. To address this, accelerated imaging techniques have been introduced by undersampling k-space, which reduces the quality of the resulting images. Recent advancements in deep learning have aimed to expedite the scanning process while maintaining the high image quality. However, deep learning models still struggle to adapt to different sampling modes, and achieving generalization across a wide range of undersampling factors remains challenging. Therefore, an effective universal model for processing random undersampling is essential and promising. In this work, we introduce UPCMR, an unrolled model designed for random sampling CMR reconstruction. This model incorporates two kinds of learnable prompts, undersampling-specific prompt and spatial-specific prompt, and combines them with the UNet structure in each block, aiming to provide an effective and versatile solution for the above challenge.},
}

@inproceedings{Chen:2024,
  abbr =    {},
  bibtex_show = {true},
  author =    {Chen, Yunjie and Wolterink, Jelmer M. and Neve, Olaf M. and Romeijn, Stephan R. and Verbist, Berit M. and Hensen, Erik F. and Tao, Qian and Staring, Marius},
  title =     {Vestibular schwannoma growth prediction from longitudinal MRI by time-conditioned neural fields},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  address =   {Marrakech, Morocco},
  series =    {Lecture Notes in Computer Science},
  volume =    {15003},
  pages =     {508 -- 518},
  month =     {October},
  year =      {2024},
  pdf =       {2024_c_MICCAI.pdf},
  html =      {https://doi.org/10.1007/978-3-031-72384-1_48},
  arxiv =     {2404.02614},
  code =      {https://github.com/cyjdswx/DeepGrowth},
  abstract =  {Vestibular schwannomas (VS) are benign tumors that are generally managed by active surveillance with MRI examination. To further assist clinical decision-making and avoid overtreatment, an accurate prediction of tumor growth based on longitudinal imaging is highly desirable. In this paper, we introduce DeepGrowth, a deep learning method that incorporates neural fields and recurrent neural networks for prospective tumor growth prediction. In the proposed model, each tumor is represented as a signed distance function (SDF) conditioned on a low-dimensional latent code. Unlike previous studies, we predict the latent codes of the future tumor and generate the tumor shapes from it using a multilayer perceptron (MLP). To deal with irregular time intervals, we introduce a time-conditioned recurrent module based on a ConvLSTM and a novel temporal encoding strategy, which enables the proposed model to output varying tumor shapes over time. The experiments on an in-house longitudinal VS dataset showed that the proposed model significantly improved the performance (&ge; 1.6% Dice score and &ge; 0.20 mm 95% Hausdorff distance), in particular for top 20% tumors that grow or shrink the most (&ge; 4.6% Dice score and &ge; 0.73 mm 95% Hausdorff distance). Our code is available at <a href="https://github.com/cyjdswx/DeepGrowth">https://github.com/cyjdswx/DeepGrowth</a>.},
}

@inproceedings{vanderValk:2023,
  abbr =    {MICCAI},
  bibtex_show = {true},
  author =    {van der Valk, Viktor and Atsma, Douwe and Scherptong, Roderick and Staring, Marius},
  title =     {Joint optimization of a {\beta}-VAE for ECG task-specific feature extraction},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  address =   {Vancouver, Canada},
  series =    {Lecture Notes in Computer Science},
  volume =    {14221},
  pages =     {554 -- 563},
  month =     {October},
  year =      {2023},
  pdf =       {2023_c_MICCAI.pdf},
  html =      {https://doi.org/10.1007/978-3-031-43895-0_52},
  arxiv =     {2304.06476},
  abstract =  {Electrocardiography is the most common method to investigate the condition of the heart through the observation of cardiac rhythm and electrical activity, for both diagnosis and monitoring purposes. Analysis of electrocardiograms (ECGs) is commonly performed through the investigation of specific patterns, which are visually recognizable by trained physicians and are known to reflect cardiac (dis)function. In this work we study the use of Î²-variational autoencoders (VAEs) as an explainable feature extractor, and improve on its predictive capacities by jointly optimizing signal reconstruction and cardiac function prediction. The extracted features are then used for cardiac function prediction using logistic regression. The method is trained and tested on data from 7255 patients, who were treated for acute coronary syndrome at the Leiden University Medical Center between 2010 and 2021. The results show that our method significantly improved prediction and explainability compared to a vanilla {\beta}-VAE, while still yielding similar reconstruction performance.}
}

@inproceedings{Ntatsis:2023,
  abbr =    {},
  bibtex_show = {true},
  author =    {Ntatsis, Konstantinos and Dekker, Niels and van der Valk, Viktor and Birdsong, Tom and Zukic, Dzenan and Klein, Stefan and Staring, Marius and McCormick, Matthew},
  title =     {itk-elastix: Medical image registration in Python},
  booktitle = {Proceedings of the 22nd Python in Science Conference},
  editor =    {Agarwal, Meghann and Calloway, Chris and Niederhut, Dillon},
  pages =     {101 - 105},
  month =     {July},
  year =      {2023},
  pdf =       {2023_c_scipy.pdf},
  html =      {http://dx.doi.org/10.25080/gerudo-f2bc6f59-00d},
  arxiv =     {},
  code =      {https://github.com/InsightSoftwareConsortium/ITKElastix},
  abstract =  {Image registration plays a vital role in understanding changes that occur in 2D and 3D scientific imaging datasets. Registration involves finding a spatial transformation that aligns one image to another by optimizing relevant image similarity metrics. In this paper, we introduce itk-elastix, a user-friendly Python wrapping of the mature elastix registration toolbox. The open-source tool supports rigid, affine, and B-spline deformable registration, making it versatile for various imaging datasets. By utilizing the modular design of itk-elastix, users can efficiently configure and compare different registration methods, and embed these in image analysis workflows.},
}

@inproceedings{Chen:2023,
  abbr =    {},
  bibtex_show = {true},
  author =    {Chen, Yunjie and Staring, Marius and Wolterink, Jelmer M. and Tao, Qian},
  title =     {Local implicit neural representations for multi-sequence MRI translation},
  booktitle = {IEEE International Symposium on Biomedical Imaging (ISBI)},
  address =   {Cartagena de Indias, Colombia},
  month =     {April},
  year =      {2023},
  pdf =       {2023_c_ISBI.pdf},
  html =      {https://doi.org/10.1109/ISBI53787.2023.10230409},
  arxiv =     {2302.01031},
  code =      {},
  abstract =  {In radiological practice, multi-sequence MRI is routinely acquired to characterize anatomy and tissue. However, due to the heterogeneity of imaging protocols and contra-indications to contrast agents, some MRI sequences, e.g. contrast-enhanced T1-weighted image (T1ce), may not be acquired. This creates difficulties for large-scale clinical studies for which heterogeneous datasets are aggregated. Modern deep learning techniques have demonstrated the capability of synthesizing missing sequences from existing sequences, through learning from an extensive multi-sequence MRI dataset. In this paper, we propose a novel MR image translation solution based on <em>local implicit neural representations</em>. We split the available MRI sequences into local patches and assign to each patch a local multi-layer perceptron (MLP) that represents a patch in the T1ce. The parameters of these local MLPs are generated by a hypernetwork based on image features. Experimental results and ablation studies on the BraTS challenge dataset showed that the local MLPs are critical for recovering fine image and tumor details, as they allow for local specialization that is highly important for accurate image translation. Compared to a classical pix2pix model, the proposed method demonstrated visual improvement and significantly improved quantitative scores (MSE 0.86 10<sup>-3</sup> vs. 1.02 10<sup>-3</sup> and SSIM 94.9 vs 94.3).},
}

@inproceedings{Tan:2023,
  abbr =    {},
  bibtex_show = {true},
  author =    {Tan, Yicong and Mody, Prerak and van der Valk, Viktor and Staring, Marius and van Gemert, Jan},
  title =     {Analyzing Components of a Transformer under Different Data Scales in 3D Prostate CT Segmentation},
  booktitle = {SPIE Medical Imaging: Computer-Aided Diagnosis},
  editor =    {Colliot, Olivier and I{\v{s}}gum, Ivana},
  address =   {San Diego, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {12464},
  pages =     {1246408},
  month =     {February},
  year =      {2023},
  pdf =       {2023_c_SPIEMI.pdf},
  html =      {http://dx.doi.org/10.1117/12.2651572},
  arxiv =     {},
  code =      {https://github.com/prerakmody/window-transformer-prostate-segmentation},
  abstract =  {Literature on medical imaging segmentation claims that hybrid UNet models containing both Transformer and convolutional blocks perform better than purely convolutional UNet models. This recently touted success of Transformers warrants an investigation into which of its components contribute to its performance. Also, previous work has a limitation of analysis only at fixed data scales as well as unfair comparisons with others models where parameter counts are not equivalent. This work investigates the performance of the window-Based Transformer for prostate CT Organ-at-Risk (OAR) segmentation at different data scales in context of replacing its various components. To compare with literature, the first experiment replaces the window-based Transformer block with convolution. Results show that the convolution prevails as the data scale increases. In the second experiment, to reduce complexity, the self-attention mechanism is replaced with an equivalent albeit simpler spatial mixing operation i.e. max-pooling. We observe improved performance for max-pooling in smaller data scales, indicating that the window-based Transformer may not be the best choice in both small and larger data scales. Finally, since convolution has an inherent local inductive bias of positional information, we conduct a third experiment to imbibe such a property to the Transformer by exploring two kinds of positional encodings. The results show that there are insignificant improvements after adding positional encoding, indicating the Transformers deficiency in capturing positional information given our data scales. We hope that our approach can serve as a framework for others evaluating the utility of Transformers for their tasks. Code is available via <a href="https://github.com/prerakmody/window-transformer-prostate-segmentation">GitHub</a>.},
}

@inproceedings{Mody:2022,
  abbr =    {},
  bibtex_show = {true},
  author =    {Mody, Prerak P. and Chaves-de-Plaza, Nicolas F. and Hildebrandt, Klaus and Staring, Marius},
  title =     {Improving Error Detection in Deep Learning based Radiotherapy Autocontours using Bayesian Uncertainty},
  booktitle = {Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, MICCAI workshop},
  address =   {Singapore},
  series =    {Lecture Notes in Computer Science},
  volume =    {13563},
  pages =     {70 - 79},
  month =     {September},
  year =      {2022},
  pdf =       {2022_c_MICCAI-UNSURE.pdf},
  html =      {https://doi.org/10.1007/978-3-031-16749-2_7},
  arxiv =     {},
  code =      {},
  abstract =  {Bayesian Neural Nets (BNN) are increasingly used for robust organ auto-contouring. Uncertainty heatmaps extracted from BNNs have been shown to correspond to inaccurate regions. To help speed up the mandatory quality assessment (QA) of contours in radiotherapy, these heatmaps could be used as stimuli to direct visual attention of clinicians to potential inaccuracies. In practice, this is non-trivial to achieve since many accurate regions also exhibit uncertainty. To influence the output uncertainty of a BNN, we propose a modified accuracy-versus-uncertainty (AvU) metric as an additional objective during model training that penalizes both accurate regions exhibiting uncertainty as well as inaccurate regions exhibiting certainty. For evaluation, we use an uncertainty-ROC curve that can help differentiate between Bayesian models by comparing the probability of uncertainty in inaccurate versus accurate regions. We train and evaluate a FlipOut BNN model on the MICCAI2015 Head and Neck Segmentation challenge dataset and on the DeepMind-TCIA dataset, and observed an increase in the AUC of uncertainty-ROC curves by 5.6% and 5.9%, respectively, when using the AvU objective. The AvU objective primarily reduced false positives regions (uncertain and accurate), drawing less visual attention to these regions, thereby potentially improving the speed of error detection.},
}

@inproceedings{Chaves-de-Plaza:2022,
  abbr =    {},
  bibtex_show = {true},
  author =    {Chaves-de-Plaza, Nicolas and Mody, Prerak P. and Hildebrandt, Klaus and de Ridder, Huib and Staring, Marius and van Egmond, Ren{\'e}},
  title =     {Towards Fast and Robust AI-Infused Human-Centered Contouring Workflows For Adaptive Proton Therapy in the Head and Neck},
  booktitle = {European Chapter of the Human Factors and Ergonomics Society},
  address =   {Turin, Italy},
  month =     {April},
  year =      {2022},
  pdf =       {2022_c_HFES.pdf},
  html =      {},
  arxiv =     {2208.04675},
  code =      {},
  abstract =  {Delineation of tumors and organs-at-risk permits detecting and correcting changes in the patients' anatomy throughout the treatment, making it a core step of adaptive proton therapy (APT). Although AI-based auto-contouring technologies have sped up this process, the time needed to perform the quality assessment (QA) of the generated contours remains a bottleneck, taking clinicians between several minutes up to an hour to complete. This paper introduces a fast contouring workflow suitable for time-critical APT, enabling detection of anatomical changes in shorter time frames and with a lower demand of clinical resources. The proposed human-centered AI-infused workflow follows two principles uncovered after reviewing the APT literature and conducting several interviews and an observational study in two radiotherapy centers in the Netherlands. First, enable targeted inspection of the generated contours by leveraging AI uncertainty and clinically-relevant features such as the proximity of the organs-at-risk to the tumor. Second, minimize the number of interactions needed to edit faulty delineations with redundancy-aware editing tools that provide the user a sense of predictability and control. We use a proof of concept that we validated with clinicians to demonstrate how current and upcoming AI capabilities support the workflow and how it would fit into clinical practice.},
}

@inproceedings{Jia:2022,
  abbr =    {},
  bibtex_show = {true},
  author =    {Jia, Jingnan and Staring, Marius and Hern{\'a}ndez Gir{\'o}n, Irene and Kroft, Lucia J.M. and Schouffoer, Anne A. and Stoel, Berend C.},
  title =     {Prediction of Lung CT Scores of Systemic Sclerosis by Cascaded Regression Neural Networks},
  booktitle = {SPIE Medical Imaging: Computer-Aided Diagnosis},
  editor =    {Colliot, Olivier and Isgum, Ivana},
  address =   {San Diego, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {12033},
  pages =     {1203338},
  month =     {February},
  year =      {2022},
  pdf =       {2022_c_SPIEMIa.pdf},
  html =      {https://doi.org/10.1117/12.2602737},
  arxiv =     {},
  code =      {},
  abstract =  {Visually scoring lung involvement in systemic sclerosis from CT scans plays an important role in monitoring progression, but its labor intensiveness hinders practical application. We proposed, therefore, an automatic scoring framework that consists of two cascaded deep regression neural networks. The first (3D) network aims to predict the craniocaudal position of five anatomically defined scoring levels on the 3D CT scans. The second (2D) network receives the resulting 2D axial slices and predicts the scores. We used 227 3D CT scans to train and validate the first network, and the resulting 1135 axial slices were used in the second network. Two experts scored independently a subset of data to obtain intra- and inter-observer variabilities and the ground truth for all data was obtained in consensus. To alleviate the unbalance in training labels in the second network, we introduced a sampling technique and to increase the diversity of the training samples synthetic data was generated, mimicking ground glass and reticulation patterns. The 4-fold cross validation showed that our proposed network achieved an average MAE of 5.90, 4.66 and 4.49, weighted kappa of 0.66, 0.58 and 0.65 for total score (TOT), ground glass (GG) and reticular pattern (RET), respectively. Our network performed slightly worse than the best experts on TOT and GG prediction but it has competitive performance on RET prediction and has the potential to be an objective alternative for the visual scoring of SSc in CT thorax studies.},
}

@inproceedings{Mody:2022,
  abbr =    {},
  bibtex_show = {true},
  author =    {Mody, Prerak P and Chaves-de-Plaza, Nicolas and Hildebrandt, Klaus and van Egmond, Ren{/'e} and Villanova, Anna and Staring, Marius},
  title =     {Comparing Bayesian Models for Organ Contouring in Head and Neck Radiotherapy},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Colliot, Olivier and Isgum, Ivana},
  address =   {San Diego, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {12032},
  pages =     {120320F},
  month =     {February},
  year =      {2022},
  pdf =       {2022_c_SPIEMIb.pdf},
  html =      {https://doi.org/10.1117/12.2611083},
  arxiv =     {},
  code =      {},
  abstract =  {Deep learning models for organ contouring in radiotherapy are poised for clinical usage, but currently, there exist few tools for automated quality assessment (QA) of the predicted contours. Bayesian models and their associated uncertainty, can potentially automate the process of detecting inaccurate predictions. We investigate two Bayesian models for auto-contouring, DropOut and FlipOut, using a quantitative measure - expected calibration error (ECE) and a qualitative measure - region-based accuracy-vs-uncertainty (R-AvU) graphs. It is well understood that a model should have low ECE to be considered trustworthy. However, in a QA context, a model should also have high uncertainty in inaccurate regions and low uncertainty in accurate regions. Such behaviour could direct visual attention of expert users to potentially inaccurate regions, leading to a speed-up in the QA process. Using R-AvU graphs, we qualitatively compare the behaviour of different models in accurate and inaccurate regions. Experiments are conducted on the MICCAI2015 Head and Neck Segmentation Challenge and on the DeepMindTCIA CT dataset using three models: DropOut-DICE, Dropout-CE (Cross Entropy) and FlipOut-CE. Quantitative results show that DropOut-DICE has the highest ECE, while Dropout-CE and FlipOut-CE have the lowest ECE. To better understand the difference between DropOut-CE and FlipOut-CE, we use the R-AvU graph which shows that FlipOut-CE has better uncertainty coverage in inaccurate regions than DropOut-CE. Such a combination of quantitative and qualitative metrics explores a new approach that helps to select which model can be deployed as a QA tool in clinical settings.},
}

@inproceedings{Li:2022,
  abbr =    {},
  bibtex_show = {true},
  author =    {Li, Yichao and Elmahdy, Mohamed S. and Lew, Michael S. and Staring, Marius},
  title =     {Transformation-Consistent Semi-Supervised Learning for Prostate CT Radiotherapy},
  booktitle = {SPIE Medical Imaging: Computer-Aided Diagnosis},
  editor =    {Colliot, Olivier and Isgum, Ivana},
  address =   {San Diego, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {12033},
  pages =     {120333O},
  month =     {February},
  year =      {2022},
  pdf =       {2022_c_SPIEMIc.pdf},
  html =      {https://doi.org/10.1117/12.2604968},
  arxiv =     {},
  code =      {},
  abstract =  {Deep supervised models often require a large amount of labelled data, which is difficult to obtain in the medical domain. Therefore, semi-supervised learning (SSL) has been an active area of research due to its promise to minimize training costs by leveraging unlabelled data. Previous research have shown that SSL is especially effective in low labelled data regimes, we show that outperformance can be extended to high data regimes by applying Stochastic Weight Averaging (SWA), which incurs zero additional training cost. Our model was trained on a prostate CT dataset and achieved improvements of 0.12 mm, 0.14 mm, 0.32 mm, and 0.14 mm for the prostate, seminal vesicles, rectum, and bladder respectively, in terms of median test set mean surface distance (MSD) compared to the supervised baseline in our high data regime.},
}

@inproceedings{Johnson:2021,
  abbr =    {},
  bibtex_show = {true},
  author =    {Johnson, Patricia M. and Jeong, Geunu and Hammernik, Kerstin and Schlemper, Jo and Qin, Chen and Duan, Jinming and Rueckert, Daniel and Lee, Jingu and Pezzotti, Nicola and De Weerdt, Elwin and Yousefi, Sahar and Elmahdy, Mohamed S. and Van Gemert, Jeroen Hendrikus Franciscus and Schuelke, Chistophe and Doneva, Mariya and Nielsen, Tim and Kastryulin, Sergey and Lelieveldt, Boudewijn P. F. and Van Osch, Matthias J. P. and Staring, Marius and Chen, Eric Z. and Wang, Puyang and Chen, Xiao and Chen, Terrence and Patel, Vishal M. and Sun, Shanhui and Shin, Hyungseob and Jun, Yohan and Eo, Taejoon and Kim, Sewon and Kim, Taeseong and Hwang, Dosik and Putzky, Patrick and Karkalousos, Dimitrios and Teuwen, Jonas and Miriakov, Nikita and Bakker, Bart and Caan, Matthan and Welling, Max and Muckley, Matthew J. and Knoll, Florian},
  title =     {Evaluation of the Robustness of Learned MR Image Reconstruction to Systematic Deviations Between Training and Test Data for the Models from the fastMRI Challenge},
  booktitle = {Machine Learning for Medical Image Reconstruction, MICCAI workshop},
  editor =    {Haq, Nandinee F. and Johnson, Patricia and Maier, Andreas and W{\"u}rfl, Tobias and Yoo, Jaejun},
  address =   {Strasbourg, France},
  series =    {Lecture Notes in Computer Science},
  volume =    {12964},
  pages =     {25 - 34},
  month =     {October},
  year =      {2021},
  pdf =       {2021_c_MICCAI-MLMIR.pdf},
  html =      {https://doi.org/10.1007/978-3-030-88552-6_3},
  arxiv =     {},
  code =      {},
  abstract =  {The 2019 fastMRI challenge was an open challenge designed to advance research in the field of machine learning for MR image reconstruction. The goal for the participants was to reconstruct undersampled MRI k-space data. The original challenge left an open question as to how well the reconstruction methods will perform in the setting where there is a systematic difference between training and test data. In this work we tested the generalization performance of the submissions with respect to various perturbations, and despite differences in model architecture and training, all of the methods perform very similarly.},
}

@inproceedings{Jia:2021,
  abbr =    {},
  bibtex_show = {true},
  author =    {Jia, Jingnan and Zhai, Zhiwei and Bakker, M. Els and Hern{\'a}ndez Gir{\'o}n, I. and Staring, Marius and Stoel, Berend C.},
  title =     {Multi-Task Semi-Supervised Learning for Pulmonary Lobe Segmentation},
  booktitle = {IEEE International Symposium on Biomedical Imaging (ISBI)},
  address =   {Nice, France},
  pages =     {1329 - 1332},
  month =     {April},
  year =      {2021},
  pdf =       {2021_c_ISBI.pdf},
  html =      {https://doi.org/10.1109/ISBI48211.2021.9433985},
  arxiv =     {},
  code =      {},
  abstract =  {Pulmonary lobe segmentation is an important preprocessing task for the analysis of lung diseases. Traditional methods relying on fissure detection or other anatomical features, such as the distribution of pulmonary vessels and airways, could provide reasonably accurate lobe segmentations. Deep learning based methods can outperform these traditional approaches, but require large datasets. Deep multi-task learning is expected to utilize labels of multiple different structures. However, commonly such labels are distributed over multiple datasets. In this paper, we proposed a multi-task semi-supervised model that can leverage information of multiple structures from unannotated datasets and datasets annotated with different structures. A focused alternating training strategy is presented to balance the different tasks. We evaluated the trained model on an external independent CT dataset. The results show that our model significantly outperforms single-task alternatives, improving the mean surface distance from 7.174 mm to 4.196 mm. We also demonstrated that our approach is successful for different network architectures as backbones.},
}

@inproceedings{Beljaards:2020,
  abbr =    {},
  bibtex_show = {true},
  author =    {Beljaards, Laurens and Elmahdy, Mohamed S. and Verbeek, Fons and Staring, Marius},
  title =     {A Cross-Stitch Architecture for Joint Registration and Segmentation in Adaptive Radiotherapy},
  booktitle = {Medical Imaging with Deep Learning},
  editor =    {Pal, Christopher and Descoteaux, Maxime},
  address =   {Montreal, Canada},
  series =    {Proceedings of Machine Learning Research},
  volume =    {121},
  pages =     {62 - 74},
  month =     {July},
  year =      {2020},
  pdf =       {2020_c_MIDL.pdf},
  html =      {http://proceedings.mlr.press/v121/beljaards20a.html},
  arxiv =     {2004.08122},
  code =      {},
  abstract =  {Recently, joint registration and segmentation has been formulated in a deep learning setting, by the definition of joint loss functions. In this work, we investigate joining these tasks at the architectural level. We propose a registration network that integrates segmentation propagation between images, and a segmentation network to predict the segmentation directly. These networks are connected into a single joint architecture via so-called cross-stitch units, allowing information to be exchanged between the tasks in a learnable manner. The proposed method is evaluated in the context of adaptive image-guided radiotherapy, using daily prostate CT imaging. Two datasets from different institutes and manufacturers were involved in the study. The first dataset was used for training (12 patients) and validation (6 patients), while the second dataset was used as an independent test set (14 patients). In terms of mean surface distance, our approach achieved 1.06 &pm; 0.3 mm, 0.91 &pm; 0.4 mm, 1.27 &pm; 0.4 mm, and 1.76 &pm; 0.8 mm on the validation set and 1.82 &pm; 2.4 mm, 2.45 &pm; 2.4 mm, 2.45 &pm; 5.0 mm, and 2.57 &pm; 2.3 mm on the test set for the prostate, bladder, seminal vesicles, and rectum, respectively. The proposed multi-task network outperformed single-task networks, as well as a network only joined through the loss function, thus demonstrating the capability to leverage the individual strengths of the segmentation and registration tasks. The obtained performance as well as the inference speed make this a promising candidate for daily re-contouring in adaptive radiotherapy, potentially reducing treatment-related side effects and improving quality-of-life after treatment.},
}

@inproceedings{Elmahdy:2020,
  abbr =    {},
  bibtex_show = {true},
  author =    {Elmahdy, Mohamed S. and Ahuja, Tanuj and van der Heide, Uulke A. and Staring, Marius},
  title =     {Patient-Specific Finetuning of Deep Learning Models for Adaptive Radiotherapy in Prostate CT},
  booktitle = {IEEE International Symposium on Biomedical Imaging (ISBI)},
  address =   {Iowa City, Iowa, USA},
  pages =     {577 - 580},
  month =     {April},
  year =      {2020},
  pdf =       {2020_c_ISBI.pdf},
  html =      {https://doi.org/10.1109/ISBI45749.2020.9098702},
  arxiv =     {},
  code =      {},
  abstract =  {Contouring of the target volume and Organs-At-Risk (OARs) is a crucial step in radiotherapy treatment planning. In an adaptive radiotherapy setting, updated contours need to be generated based on daily imaging. In this work, we leverage personalized anatomical knowledge accumulated over the treatment sessions, to improve the segmentation accuracy of a pre-trained Convolution Neural Network (CNN), for a specific patient. We investigate a transfer learning approach, finetuning the baseline CNN model to a specific patient, based on imaging acquired in earlier treatment fractions. The baseline CNN model is trained on a prostate CT dataset from one hospital of 379 patients. This model is then fine-tuned and tested on an independent dataset of another hospital of 18 patients, each having 7 to 10 daily CT scans. For the prostate, seminal vesicles, bladder and rectum, the model fine-tuned on each specific patient achieved a Mean Surface Distance (MSD) of 1:64 &pm; 0:43 mm, 2:38 &pm; 2:76 mm, 2:30 &pm; 0:96 mm, and 1:24 &pm; 0:89 mm, respectively, which was significantly better than the baseline model. The proposed personalized model adaptation is therefore very promising for clinical implementation in the context of adaptive radiotherapy of prostate cancer.},
}

@inproceedings{deVos:2020,
  abbr =    {},
  bibtex_show = {true},
  author =    {de Vos, Bob D. and van der Velden, Bas H. M. and Sander, J{\"o}rg and Gilhuijs, Kenneth G.A. and Staring, Marius and I{\v{s}}gum, Ivana},
  title =     {Mutual information for unsupervised deep learning image registration},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {I{\v{s}}gum, Ivana and Landman, Bennett A.},
  address =   {Houston, Texas, USA},
  series =    {Proceedings of SPIE},
  volume =    {11313},
  pages =     {11313OR},
  month =     {February},
  year =      {2020},
  pdf =       {2020_c_SPIEMI.pdf},
  html =      {https://doi.org/10.1117/12.2549729},
  arxiv =     {},
  code =      {},
  abstract =  {Current unsupervised deep learning-based image registration methods are trained with mean squares or normalized cross correlation as a similarity metric. These metrics are suitable for registration of images where a linear relation between image intensities exists. When such a relation is absent knowledge from conventional image registration literature suggests the use of mutual information. In this work we investigate whether mutual information can be used as a loss for unsupervised deep learning image registration by evaluating it on two datasets: breast dynamic contrast-enhanced MR and cardiac MR images. The results show that training with mutual information as a loss gives on par performance compared with conventional image registration in contrast enhanced images, and the results show that it is generally applicable since it has on par performance compared with normalized cross correlation in single-modality registration.},
}

@inproceedings{Zhai:2019,
  abbr =    {},
  bibtex_show = {true},
  author =    {Zhai, Zhiwei and Staring, Marius and Zhou, Xuhui and Xie, Qiuxia and Xiao, Xiaojuan and Bakker, M. Els and Kroft, Lucia J. and Lelieveldt, Boudewijn P.F. and Boon, Duliette and Klok, Frederikus A. and Stoel, Berend C.},
  title =     {Linking convolutional neural networks with graph convolutional networks: application in pulmonary artery-vein separation},
  booktitle = {Graph Learning in Medical Imaging, MICCAI workshop},
  editor =    {Zhang, D. and Zhou, L. and Jie, B. and Liu, M.},
  address =   {Shenzhen, China},
  series =    {Lecture Notes in Computer Science},
  volume =    {11849},
  pages =     {36 - 43},
  month =     {October},
  year =      {2019},
  pdf =       {2019_c_MICCAI-GLMI.pdf},
  html =      {https://doi.org/10.1007/978-3-030-35817-4_5},
  arxiv =     {},
  code =      {},
  abstract =  {Graph Convolutional Networks (GCNs) are a novel and powerful method for dealing with non-Euclidean data, while Convolutional Neural Networks (CNNs) can learn features from Euclidean data such as images. In this work, we propose a novel method to combine CNNs with GCNs (CNN-GCN), that can consider both Euclidean and non-Euclidean features and can be trained end-to-end. We applied this method to separate the pulmonary vascular trees into arteries and veins (A/V). Chest CT scans were pre-processed by vessel segmentation and skeletonization, from which a graph was constructed: voxels on the skeletons resulting in a vertex set and their connections in an adjacency matrix. 3D patches centered around each vertex were extracted from the CT scans, oriented perpendicularly to the vessel. The proposed CNN-GCN classifier was trained and applied on the constructed vessel graphs, where each node is then labeled as artery or vein. The proposed method was trained and validated on data from one hospital (11 patient, 22 lungs), and tested on independent data from a different hospital (10 patients, 10 lungs). A baseline CNN method and human observer performance were used for comparison. The CNN-GCN method obtained a median accuracy of 0.773 (0.738) in the validation (test) set, compared to a median accuracy of 0.817 by the observers, and 0.727 (0.693) by the CNN. In conclusion, the proposed CNN-GCN method combines local image information with graph connectivity information, improving pulmonary A/V separation over a baseline CNN method, approaching the performance of human observers.},
}

@inproceedings{Yousefi:2019,
  abbr =    {},
  bibtex_show = {true},
  author =    {Yousefi, Sahar and Hirschler, L. and van der Plas, M. and Mohamed Elmahdi, and Sokooti, Hessam and van Osch, Mathias J.P. and Staring, Marius},
  title =     {Fast Dynamic Perfusion and Angiography Reconstruction using an end-to-end 3D Convolutional Neural Network},
  booktitle = {Machine Learning for Medical Image Reconstruction, MICCAI workshop},
  editor =    {Knoll, Florian and Maier, Andreas and Rueckert, Daniel and Ye, Jong Chul},
  address =   {Shenzhen, China},
  series =    {Lecture Notes in Computer Science},
  volume =    {11905},
  pages =     {25 - 35},
  month =     {October},
  year =      {2019},
  pdf =       {2019_c_MICCAI-MLMIR.pdf},
  html =      {https://doi.org/10.1007/978-3-030-33843-5_3},
  arxiv =     {},
  code =      {},
  abstract =  {Hadamard time-encoded pseudo-continuous arterial spin labeling (te-pCASL) is a signal-to-noise ratio (SNR)-efficient MRI technique for acquiring dynamic pCASL signals that encodes the temporal information into the labeling according to a Hadamard matrix. In the decoding step, the contribution of each sub-bolus can be isolated resulting in dynamic perfusion scans. When acquiring te-ASL both with and without flow-crushing, the ASL-signal in the arteries can be isolated resulting in 4D-angiographic information. However, obtaining multi-timepoint perfusion and angiographic data requires two acquisitions. In this study, we propose a 3D Dense-Unet convolutional neural network with a multilevel loss function for reconstructing multi-timepoint perfusion and angiographic information from an interleaved 50%-sampled crushed and 50%-sampled non-crushed data, thereby negating the additional scan time. We present a framework to generate dynamic pCASL training and validation data, based on models of the intravascular and extravascular te-pCASL signals. The proposed network achieved SSIM values of 97.3 &pm; 1.1 and 96.2 &pm; 11.1 respectively for 4D perfusion and angiographic data reconstruction for 313 test data-sets.},
}

@inproceedings{Elmahdy:2019,
  abbr =    {},
  bibtex_show = {true},
  author =    {Elmahdy, Mohamed S. and Wolterink, Jelmer M. and Sokooti, Hessam and I{\v{s}}gum, Ivana and Staring, Marius},
  title =     {Adversarial optimization for joint registration and segmentation in prostate CT radiotherapy},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  editor =    {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
  address =   {Shenzhen, China},
  series =    {Lecture Notes in Computer Science},
  volume =    {11769},
  pages =     {366 - 374},
  month =     {October},
  year =      {2019},
  pdf =       {2019_c_MICCAI.pdf},
  html =      {https://doi.org/10.1007/978-3-030-32226-7_41},
  arxiv =     {1906.12223},
  code =      {},
  abstract =  {Joint image registration and segmentation has long been an active area of research in medical imaging. Here, we reformulate this problem in a deep learning setting using adversarial learning. We consider the case in which fixed and moving images as well as their segmentations are available for training, while segmentations are not available during testing; a common scenario in radiotherapy. The proposed framework consists of a 3D end-to-end generator network that estimates the deformation vector field (DVF) between fixed and moving images in an unsupervised fashion and applies this DVF to the moving image and its segmentation. A discriminator network is trained to evaluate how well the moving image and segmentation align with the fixed image and segmentation. The proposed network was trained and evaluated on follow-up prostate CT scans for image-guided radiotherapy, where the planning CT contours are propagated to the daily CT images using the estimated DVF. A quantitative comparison with conventional registration using elastix showed that the proposed method improved performance and substantially reduced computation time, thus enabling real-time contour propagation necessary for online-adaptive radiotherapy.},
}

@inproceedings{Marstal:2019,
  abbr =    {},
  bibtex_show = {true},
  author =    {Marstal, Kasper and Berendsen, Floris and Dekker, Niels and Staring, Marius and Klein, Stefan},
  title =     {The Continuous Registration Challenge: Evaluation-As-A-Service for Medical Image Registration Algorithms},
  booktitle = {IEEE International Symposium on Biomedical Imaging (ISBI)},
  address =   {Venice, Italy},
  pages =     {1399 - 1402},
  month =     {April},
  year =      {2019},
  pdf =       {2019_c_ISBI.pdf},
  html =      {https://doi.org/10.1109/ISBI.2019.8759559},
  arxiv =     {},
  code =      {},
  abstract =  {We have developed an open source, collaborative platform for researchers to develop, compare, and improve medical image registration algorithms. The platform handles data management, unit testing, and benchmarking of registration methods in a fully automatic fashion. In this paper we describe the platform and present the Continuous Registration Challenge. The challenge focuses on registration of lung CT and brain MR images and includes eight publicly available data sets. The platform is made available to the community as an open source project and can be used for organization of future challenges.},
}

@inproceedings{Zhai:2018,
  abbr =    {},
  bibtex_show = {true},
  author =    {Zhai, Zhiwei and Staring, Marius and Ota, Hideki and Stoel, Berend C.},
  title =     {Pulmonary vessel tree matching for quantifying changes in vascular morphology},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  editor =    {Frangi, A.F. and Schnabel, J.A. and Davatzikos, C. and Alberola-Lopez, C. and Fichtinger, G.},
  address =   {Granada,Spain},
  series =    {Lecture Notes in Computer Science},
  volume =    {11071},
  pages =     {517 - 524},
  month =     {September},
  year =      {2018},
  pdf =       {2018_c_MICCAIb.pdf},
  html =      {http://dx.doi.org/10.1007/978-3-030-00934-2_58},
  arxiv =     {},
  code =      {},
  abstract =  {Invasive right-sided heart catheterization (RHC) is currently the gold standard for assessing treatment effects in pulmonary vascular diseases, such as chronic thromboembolic pulmonary hypertension (CTEPH). Quantifying morphological changes by matching vascular trees (pre- and post-treatment) may provide a non-invasive alternative for assessing hemodynamic changes. In this work, we propose a method for quantifying morphological changes, consisting of three steps: constructing vascular trees from the detected pulmonary vessels, matching vascular trees with preserving local tree topology, and quantifying local morphological changes based on Poiseuille's law (changes in radius<sup>-4</sup>, {\Delta}r<sup>-4</sup>). Subsequently, median and interquartile range (IQR) of all local {\Delta}r<sup>-4</sup> were calculated as global measurements for assessing morphological changes. The vascular tree matching method was validated with 10 synthetic trees and the relation between clinical RHC parameters and quantifications of morphological changes was investigated in 14 CTEPH patients, pre- and post-treatment. In the evaluation with synthetic trees, the proposed method achieved an average residual distance of 3:09 &pm; 1:28 mm, which is a substantial improvement over a coherent point drift method (4:32 &pm; 1:89 mm) and a method with global-local topology preservation (3:92 &pm; 1:59 mm). In the clinical evaluation, the morphological changes (IQR of {\Delta}r<sup>-4</sup>) was significantly correlated with the changes in RHC examinations, {\Delta}sPAP (R=-0.62, p-value=0.019) and {\Delta}mPAP (R=-0.56, p-value=0.038). Quantifying morphological changes may provide a noninvasive assessment of treatment effects in CTEPH patients, consistent with hemodynamic changes from invasive RHC.},
}

@inproceedings{Yousefi:2018,
  abbr =    {},
  bibtex_show = {true},
  author =    {Yousefi, Sahar and Sokooti, Hessam and Elmahdy, Mohamed S. and Peters, Femke P. and Manzuri Shalmani, Mohammad T. and Zinkstok, Roel T. and Staring, Marius},
  title =     {Esophageal Gross Tumor Volume Segmentation using a 3D Convolutional Neural Network},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  editor =    {Frangi, A.F. and Schnabel, J.A. and Davatzikos, C. and Alberola-Lopez, C. and Fichtinger, G.},
  address =   {Granada,Spain},
  series =    {Lecture Notes in Computer Science},
  volume =    {11073},
  pages =     {343 - 351},
  month =     {September},
  year =      {2018},
  pdf =       {2018_c_MICCAIa.pdf},
  html =      {http://dx.doi.org/10.1007/978-3-030-00937-3_40},
  arxiv =     {},
  code =      {},
  abstract =  {Accurate gross tumor volume (GTV) segmentation in esophagus CT images is a critical task in computer aided diagnosis (CAD) systems. However, because of the difficulties raised by the contrast similarity between esophageal GTV and its neighbouring tissues in CT scans, this problem has been addressed weakly. In this paper we present a 3D end-to-end method based on a convolutional neural network (CNN) for this purpose. We leverages design elements from DenseNet in a typical U-shape. The proposed architecture consists of a contractile path and an extending path that includes dense blocks for extracting contextual features and retrieves the lost resolution respectively. Using dense blocks leads to deep supervision, feature re-usability, and parameter reduction while aiding the network to be more accurate. The proposed architecture was trained and tested on a dataset containing 553 scans from 49 distinct patients. The proposed network achieved a Dice value of 0:73 &pm; 0:20, and a 95% mean surface distance of 3:07 &pm; 1:86 mm for 85 test scans. The experimental results indicates the effectiveness of the proposed method for clinical diagnosis and treatment systems.},
}

@inproceedings{Elmahdy:2018,
  abbr =    {},
  bibtex_show = {true},
  author =    {Elmahdy, Mohamed S. and Jagt, Thyrza and Hoogeman, Mischa S. and Zinkstok, Roel and Staring, Marius},
  title =     {Evaluation of multi-metric registration for online adaptive proton therapy of prostate cancer},
  booktitle = {International Workshop on Biomedical Image Registration (WBIR)},
  editor =    {Klein, Stefan and Staring, Marius and Durrleman, Stanley and Sommer, Stefan Horst},
  address =   {Leiden, The Netherlands},
  series =    {Lecture Notes in Computer Science},
  volume =    {10883},
  pages =     {94 - 104},
  month =     {June},
  year =      {2018},
  pdf =       {2018_c_WBIR.pdf},
  html =      {},
  arxiv =     {},
  code =      {},
  abstract =  {Delineation of the target volume and Organs-At-Risk (OARs) is a crucial step for proton therapy dose planning of prostate cancer. Adaptive proton therapy mandates automatic delineation, as manual delineation is too time consuming while it should be fast and robust. In this study, we propose an accurate and robust automatic propagation of the delineations from the planning CT to the daily CT by means of Deformable Image Registration (DIR). The proposed algorithm is a multi-metric DIR method that jointly optimizes the registration of the bladder contours and CT images. A 3D Dilated Convolutional Neural Network (DCNN) was trained for automatic bladder segmentation of the daily CT. The network was trained and tested on prostate data of 18 patients, each having 7 to 10 daily CT scans. The network achieved a Dice Similarity Coefficient (DSC) of 92.7% &pm; 1.6% for automatic bladder segmentation. For the automatic contour propagation of the prostate, lymph nodes, and seminal vesicles, the system achieved a DSC of 0.87 &pm; 0.03, 0.89 &pm; 0.02, and 0.67 &pm; 0.11 and Mean Surface Distance of 1.4 &pm; 0.30 mm, 1.4 &pm; 0.29 mm, and 1.5 &pm; 0.37 mm, respectively. The proposed algorithm is therefore very promising for clinical implementation in the context of online adaptive proton therapy of prostate cancer.},
}

@inproceedings{Bhosale:2018,
  abbr =    {},
  bibtex_show = {true},
  author =    {Bhosale, Parag and Staring, Marius and Al-Ars, Zaid and Berendsen, Floris F.},
  title =     {GPU-based stochastic-gradient optimization for non-rigid medical image registration in time-critical applications},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Angelini, Elsa A. and Landman, Bennett A.},
  address =   {Houston, Texas, USA},
  series =    {Proceedings of SPIE},
  volume =    {10574},
  pages =     {105740R},
  month =     {February},
  year =      {2018},
  pdf =       {2018_c_SPIEMI.pdf},
  html =      {http://dx.doi.org/10.1117/12.2293098},
  arxiv =     {},
  code =      {https://github.com/SuperElastix/NiftyRegSGD},
  abstract =  {Currently, non-rigid image registration algorithms are too computationally intensive to use in time-critical applications. Existing implementations that focus on speed typically address this by either parallelization on GPU-hardware, or by introducing methodically novel techniques into CPU-oriented algorithms. Stochastic gradient descent (SGD) optimization and variations thereof have proven to drastically reduce the computational burden for CPU-based image registration, but have not been successfully applied in GPU hardware due to its stochastic nature. This paper proposes 1) NiftyRegSGD, a SGD optimization for the GPU-based image registration tool NiftyReg, 2) random chunk sampler, a new random sampling strategy that better utilizes the memory bandwidth of GPU hardware. Experiments have been performed on 3D lung CT data of 19 patients, which compared NiftyRegSGD (with and without random chunk sampler) with CPU-based elastix Fast Adaptive SGD (FASGD) and NiftyReg. The registration runtime was 21.5s, 4.4s and 2.8s for elastix-FASGD, NiftyRegSGD without, and NiftyRegSGD with random chunk sampling, respectively, while similar accuracy was obtained. Our method is publicly available at <a href="https://github.com/SuperElastix/NiftyRegSGD">https://github.com/SuperElastix/NiftyRegSGD</a>.},
}

@inproceedings{deVos:2017,
  abbr =    {},
  bibtex_show = {true},
  author =    {de Vos, Bob D. and Berendsen, Floris and Viergever, Max A. and Staring, Marius and I{\v{s}}gum, Ivana},
  title =     {End-to-End Unsupervised Deformable Image Registration with a Convolutional Neural Network},
  booktitle = {Deep Learning in Medical Image Analysis Workshop at MICCAI},
  editor =    {Cardoso, M. Jorge and Arbel, Tal and Carneiro, Gustavo and Syeda-Mahmood, Tanveer and Tavares, Jo{\"a}o Manuel R.S. and Moradi, Mehdi and Bradley, Andrew and Greenspan, Hayit and Papa, Jo{\"a}o Paulo and Madabhushi, Anant and Nascimento, Jacinto C. and Cardoso, Jaime S. and Belagiannis, Vasileios and Lu, Zhi},
  address =   {Quebec,Canada},
  series =    {Lecture Notes in Computer Science},
  volume =    {10553},
  pages =     {204 - 212},
  month =     {September},
  year =      {2017},
  pdf =       {2017_c_MICCAIb.pdf},
  html =      {https://doi.org/10.1007/978-3-319-67558-9_24},
  arxiv =     {},
  code =      {},
  abstract =  {In this work we propose a deep learning network for deformable image registration (DIRNet). The DIRNet consists of a convolutional neural network (ConvNet) regressor, a spatial transformer, and a resampler. The ConvNet analyzes a pair of fixed and moving images and outputs parameters for the spatial transformer, which generates the displacement vector field that enables the resampler to warp the moving image to the fixed image. The DIRNet is trained end-to-end by unsupervised optimization of a similarity metric between input image pairs. A trained DIRNet can be applied to perform registration on unseen image pairs in one pass, thus non-iteratively. Evaluation was performed with registration of images of handwritten digits (MNIST) and cardiac cine MR scans (Sunnybrook Cardiac Data). The results demonstrate that registration with DIRNet is as accurate as a conventional deformable image registration method with short execution times.},
}

@inproceedings{Sokooti:2017,
  abbr =    {},
  bibtex_show = {true},
  author =    {Sokooti, Hessam and de Vos, Bob and Berendsen, Floris and Lelieveldt, Boudewijn P.F. and I{\v{s}}gum, Ivana and Staring, Marius},
  title =     {Nonrigid Image Registration Using Multi-Scale 3D Convolutional Neural Networks},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  editor =    {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D. Louis and Duchesne, Simon},
  address =   {Quebec,Canada},
  series =    {Lecture Notes in Computer Science},
  volume =    {10433},
  pages =     {232 - 239},
  month =     {September},
  year =      {2017},
  pdf =       {2017_c_MICCAIa.pdf},
  html =      {https://doi.org/10.1007/978-3-319-66182-7_27},
  arxiv =     {},
  code =      {},
  abstract =  {In this paper we propose a method to solve nonrigid image registration through a learning approach, instead of via iterative optimization of a predefined dissimilarity metric. We design a Convolutional Neural Network (CNN) architecture that, in contrast to all other work, directly estimates the displacement vector field (DVF) from a pair of input images. The proposed RegNet is trained using a large set of artificially generated DVFs, does not explicitly define a dissimilarity metric, and integrates image content at multiple scales to equip the network with contextual information. At testing time nonrigid registration is performed in a single shot, in contrast to current iterative methods. We tested RegNet on 3D chest CT follow-up data. The results show that the accuracy of RegNet is on par with a conventional B-spline registration, for anatomy within the capture range. Training RegNet with artificially generated DVFs is therefore a promising approach for obtaining good results on real clinical data, thereby greatly simplifying the training problem. Deformable image registration can therefore be successfully casted as a learning problem.},
}

@inproceedings{Sokooti:2016,
  abbr =    {},
  bibtex_show = {true},
  author =    {Sokooti, Hessam and Saygili, Gorkem and Glocker, Ben and Lelieveldt, Boudewijn P.F. and Staring, Marius},
  title =     {Accuracy Estimation for Medical Image Registration Using Regression Forests},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  editor =    {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
  address =   {Athens,Greece},
  series =    {Lecture Notes in Computer Science},
  volume =    {9902},
  pages =     {107 - 115},
  month =     {October},
  year =      {2016},
  pdf =       {2016_c_MICCAI.pdf},
  html =      {http://dx.doi.org/10.1007/978-3-319-46726-9%2013},
  arxiv =     {},
  code =      {},
  abstract =  {This paper reports a new automatic algorithm to estimate the misregistration in a quantitative manner. A random regression forest is constructed, predicting the local registration error. The forest is built using local and modality independent features related to the registration precision, the transformation model and intensity-based similarity after registration. The forest is trained and tested using manually annotated corresponding points between pairs of chest CT scans. The results show that the mean absolute error of regression is 0.72 &pm; 0.96 mm and the accuracy of classiffication in three classes (correct, poor and wrong registration) is 93.4%, comparing favorably to a competing method. In conclusion, a method was proposed that for the first time shows the feasibility of automatic registration assessment by means of regression, and promising results were obtained.},
}

@inproceedings{Marstal:2016,
  abbr =    {},
  bibtex_show = {true},
  author =    {Marstal, Kasper and Berendsen, Floris and Staring, Marius and Klein, Stefan},
  title =     {SimpleElastix: A user-friendly, multi-lingual library for medical image registration},
  booktitle = {International Workshop on Biomedical Image Registration (WBIR)},
  editor =    {Schnabel, Julia and Mori, Kensaku},
  address =   {Las Vegas, Nevada, USA},
  series =    {IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages =     {574 - 582},
  month =     {July},
  year =      {2016},
  pdf =       {2016_c_WBIRa.pdf},
  html =      {http://dx.doi.org/10.1109/CVPRW.2016.78},
  arxiv =     {},
  code =      {https://github.com/kaspermarstal/SimpleElastix},
  abstract =  {In this paper we present SimpleElastix, an extension of SimpleITK designed to bring the Elastix medical image registration library to a wider audience. Elastix is a modular collection of robust C++ image registration algorithms that is widely used in the literature. However, its command-line interface introduces overhead during prototyping, experimental setup, and tuning of registration algorithms. By integrating Elastix with SimpleITK, Elastix can be used as a native library in Python, Java, R, Octave, Ruby, Lua, Tcl and C# on Linux, Mac and Windows. This allows Elastix to intregrate naturally with many development environments so the user can focus more on the registration problem and less on the underlying C++ implementation. As means of demonstration, we show how to register MR images of brains and natural pictures of faces using minimal amount of code. SimpleElastix is open source, licensed under the permissive Apache License Version 2.0 and available at <a href="https://github.com/kaspermarstal/SimpleElastix">https://github.com/kaspermarstal/SimpleElastix</a>.},
}

@inproceedings{Berendsen:2016,
  abbr =    {},
  bibtex_show = {true},
  author =    {Berendsen, Floris and Marstal, Kasper and Klein, Stefan and Staring, Marius},
  title =     {The design of SuperElastix - a unifying framework for a wide range of image registration methodologies},
  booktitle = {International Workshop on Biomedical Image Registration (WBIR)},
  editor =    {Schnabel, Julia and Mori, Kensaku},
  address =   {Las Vegas, Nevada, USA},
  series =    {IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages =     {498 - 506},
  month =     {July},
  year =      {2016},
  pdf =       {2016_c_WBIRb.pdf},
  html =      {https://doi.org/10.1109/CVPRW.2016.69},
  arxiv =     {},
  code =      {https://github.com/kaspermarstal/SuperElastix},
  abstract =  {A large diversity of image registration methodologies has emerged from the research community. The scattering of methods over toolboxes impedes rigorous comparison to select the appropriate method for a given application. Toolboxes typically tailor their implementations to a mathematical registration paradigm, which makes internal functionality nonexchangeable. Subsequently, this forms a barrier for adoption of registration technology in the clinic. We therefore propose a unifying, role-based software design that can integrate a broad range of functional registration components. These components can be configured into an algorithmic network via a single high-level user interface. A generic component handshake mechanism provides users feedback on incompatibilities. We demonstrate the viability of our design by incorporating two paradigms from different code bases. The implementation is done in C++ and is available as open source. The progress of embedding more paradigms can be followed via <a href="https://github.com/kaspermarstal/SuperElastix">https://github.com/kaspermarstal/SuperElastix</a>.},
}

@inproceedings{Uilkema:2016,
  abbr =    {},
  bibtex_show = {true},
  author =    {Uilkema, Sander and van der Heide, Uulke A. and Sonke, Jan-Jakob and Staring, Marius and Nijkamp, Jasper},
  title =     {MR-based contour propagation for rectal cancer patients},
  booktitle = {18th International Conference on the use of Computers in Radiation Therapy},
  editor =    {Oelfke, Uwe and Partridge, Mike},
  address =   {London, UK},
  month =     {June},
  year =      {2016},
  pdf =       {2016_c_ICCR.pdf},
  html =      {},
  arxiv =     {},
  code =      {},
  abstract =  {In rectal cancer patients large day-to-day target volume deformations occur, leading to large PTV margins. The introduction of MR-guided RT with excellent soft-tissue contrast, facilitates adaptive procedures, like online re-planning with smaller margins. Time constraints demand for automatic contouring of the daily-MR. A possible fast solution is contour propagation with deformable image-registration (DIR). In rectal cancer patients DIR is challenging because of large local deformations of the CTV (meso-rectum) caused by (dis)appearing rectal and bladder content. To deal with this challenge, pre-treatment delineations can be used to define a region-of-interest (ROI) to limit DIR to the part of the anatomy, and also allows excluding regions with (dis-)appearing content. In this study, we investigate optimal parameter settings of MR-to-MR DIR, in the context of contour-propagation of the meso-rectum.},
}

@inproceedings{Zhai:2016,
  abbr =    {},
  bibtex_show = {true},
  author =    {Zhai, Zhiwei and Staring, Marius and Stoel, Berend C.},
  title =     {Lung vessel segmentation in CT images using graph cuts},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Styner, Martin A. and Angelini, Elsa A.},
  address =   {San Diego, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {9784},
  pages =     {97842K - 97842K-8},
  month =     {February},
  year =      {2016},
  pdf =       {2016_c_SPIEMI.pdf},
  html =      {http://dx.doi.org/10.1117/12.2216827},
  arxiv =     {},
  code =      {},
  abstract =  {Accurate lung vessel segmentation is an important operation for lung CT analysis. Hessian-based filters are popular for pulmonary vessel enhancement. However, due to their low response at vessel bifurcations and vessel boundaries, extracting lung vessels by thresholding the vesselness is inaccurate. Some literature turns to graph cuts for more accurate segmentation, as it incorporates neighbourhood information. In this work, we propose a new graph cuts cost function combining appearance and shape, where CT intensity represents appearance and vesselness from a Hessian-based filter represents shape. In order to make the graph representation computationally tractable, voxels that are considered clearly background are removed using a low threshold on the vesselness map. The graph structure is then established based on the neighbourhood relationship of the remaining voxels. Vessels are segmented by minimizing the energy cost function with the graph cuts optimization framework. We optimized the parameters and evaluated the proposed method with two manually labeled sub-volumes. For independent evaluation, we used the 20 CT scans of the VESSEL12 challenge. The evaluation results of the sub-volumes dataset show that the proposed method produced a more accurate vessels segmentation. For the VESSEL12 dataset, our method obtained a competitive performance with an area under the ROC of 0.975, especially among the binary submissions.},
}

@inproceedings{Qiao:2015,
  abbr =    {},
  bibtex_show = {true},
  author =    {Qiao, Yuchuan and Sun, Z. and Lelieveldt, Boudewijn P.F. and Staring, Marius},
  title =     {A Stochastic Quasi-Newton Method for Non-rigid Image Registration},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  editor =    {Navab, N. and Hornegger, J. and Wells, W.M. and Frangi, A.F.},
  address =   {Munchen,Germany},
  series =    {Lecture Notes in Computer Science},
  volume =    {9350},
  pages =     {297 - 304},
  month =     {September},
  year =      {2015},
  pdf =       {2015_c_MICCAI.pdf},
  html =      {http://dx.doi.org/10.1007/978-3-319-24571-3_36},
  arxiv =     {},
  code =      {},
  abstract =  {Image registration is often very slow because of the high dimensionality of the images and complexity of the algorithms. Adaptive stochastic gradient descent (ASGD) outperforms deterministic gradient descent and even quasi-Newton in terms of speed. This method, however, only exploits first-order information of the cost function. In this paper, we explore a stochastic quasi-Newton method (s-LBFGS) for non-rigid image registration. It uses the classical limited memory BFGS method in combination with noisy estimates of the gradient. Curvature information of the cost function is estimated once every L iterations and then used for the next L iterations in combination with a stochastic gradient. The method is validated on follow-up data of 3D chest CT scans (19 patients), using a B-spline transformation model and a mutual information metric. The experiments show that the proposed method is robust, efficient and fast. s-LBFGS obtains a similar accuracy as ASGD and deterministic LBFGS. Compared to ASGD the proposed method uses about 5 times fewer iterations to reach the same metric value, resulting in an overall reduction in run time of a factor of two. Compared to deterministic LBFGS, s-LBFGS is almost 500 times faster.},
}

@inproceedings{Sun:2015,
  abbr =    {},
  bibtex_show = {true},
  author =    {Sun, Zhuo and Lelieveldt, Boudewijn P.F. and Staring, Marius},
  title =     {Fast Linear Geodesic Shape Regression Using Coupled Logdemons Registration},
  booktitle = {IEEE International Symposium on Biomedical Imaging (ISBI)},
  editor =    {Angelini, Elsa and Kovacevic, Jelena},
  address =   {New York, USA},
  pages =     {1276 - 1279},
  month =     {April},
  year =      {2015},
  pdf =       {2015_c_ISBI.pdf},
  html =      {http://dx.doi.org/10.1109/ISBI.2015.7164107},
  arxiv =     {},
  code =      {},
  abstract =  {Longitudinal brain image series offers the possibility to study individual brain anatomical changes over time. Mathematical models are needed to study such developmental trajectories in detail. In this paper, we present a novel approach to study the individual brain anatomy over time via a linear geodesic shape regression method. In our method, we integrate separate pairwise registrations between the baseline image and the follow-up images into a unified spatial registration plus temporal regression framework. Different from previous geodesic shape regression approaches, which use the LDDMM framework to estimate the brain anatomical change over time, our method is based on the LogDemons method to decrease the computation cost, while maintaining the diffeomorphic property of the deformation over time. Moreover, a temporal regression constraint is explicitly implemented in each optimization iteration to make sure that the entire developmental trajectory can be compactly represented by the baseline image and an optimal stationary velocity field. Our method is mathematically well founded in the Alternating Direction Method of Multipliers (ADMM), which for our image regression application is interpreted in diffeomorphic space instead of Euclidean space. We evaluate our new method on 2D synthetic images and real 3D brain longitudinal image series, and the experiments show promising results in regression accuracy as well as estimated deformations.},
}

@inproceedings{Kitslaar:2015,
  abbr =    {},
  bibtex_show = {true},
  author =    {Kitslaar, Pieter H. and van 't Klooster, Ronald and Staring, Marius and Lelieveldt, Boudewijn P.F. and van der Geest, Rob J.},
  title =     {Segmentation of Branching Vascular Structures using Adaptive Subdivision Surface Fitting},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Ourselin, Sebastien and Styner, Martin A.},
  address =   {Orlando, Florida, USA},
  series =    {Proceedings of SPIE},
  volume =    {9413},
  pages =     {94133Z},
  month =     {February},
  year =      {2015},
  pdf =       {2015_c_SPIEMI.pdf},
  html =      {http://dx.doi.org/10.1117/12.2082222},
  arxiv =     {},
  code =      {},
  abstract =  {This paper describes a novel method for segmentation and modeling of branching vessel structures in medical images using adaptive subdivision surfaces fitting. The method starts with a rough initial skeleton model of the vessel structure. A coarse triangular control mesh consisting of hexagonal rings and dedicated bifurcation elements is constructed from this skeleton. Special attention is paid to ensure a topological sound control mesh is created around the bifurcation areas. Then, a smooth tubular surface is obtained from this coarse mesh using a standard subdivision scheme. This subdivision surface is iteratively fitted to the image. During the fitting, the target update locations of the subdivision surface are obtained using a scanline search along the surface normals, finding the maximum gradient magnitude (of the imaging data). In addition to this surface fitting framework, we propose an adaptive mesh refinement scheme. In this step the coarse control mesh topology is updated based on the current segmentation result, enabling adaptation to varying vessel lumen diameters. This enhances the robustness and flexibility of the method and reduces the amount of prior knowledge needed to create the initial skeletal model. The method was applied to publicly available CTA data from the Carotid Bifurcation Algorithm Evaluation Framework resulting in an average dice index of 89.2% with the ground truth. Application of the method to the complex vascular structure of a coronary artery tree in CTA and to MRI images were performed to show the versatility and flexibility of the proposed framework.},
}

@inproceedings{Smit:2014,
  abbr =    {},
  bibtex_show = {true},
  author =    {Smit, Noeska N. and Klein Haneveld, Berend and Staring, Marius and Eisemann, Elmar and Botha, Charl P. and Vilanova, Anna},
  title =     {RegistrationShop: An Interactive 3D Medical Volume Registration System},
  booktitle = {Eurographics Workshop on Visual Computing for Biology and Medicine},
  editor =    {Viola, I and Buehler, K. and Ropinski, T.},
  address =   {Vienna, Austria},
  pages =     {145 - 153},
  month =     {September},
  year =      {2014},
  pdf =       {2014_c_VCBM.pdf},
  html =      {http://dx.doi.org/10.2312/vcbm.20141193},
  arxiv =     {},
  code =      {https://github.com/berendkleinhaneveld/Registrationshop},
  abstract =  {In medical imaging, registration is used to combine images containing information from different modalities or to track treatment effects over time in individual patients. Most registration software packages do not provide an easy-to-use interface that facilitates the use of registration. 2D visualization techniques are often used for visualizing 3D datasets.<br>RegistrationShop was developed to improve and ease the process of volume registration using 3D visualizations and intuitive interactive tools. It supports several basic visualizations of 3D volumetric data. Interactive rigid and non-rigid transformation tools can be used to manipulate the volumes and immediate visual feedback for all rigid transformation tools allows the user to examine the current result in real-time. In this context, we introduce 3D comparative visualization techniques, as well as a way of placing landmarks in 3D volumes. Finally, we evaluated our approach with domain experts, who underlined the potential and usefulness of RegistrationShop.},
}

@inproceedings{Qiao:2014,
  abbr =    {},
  bibtex_show = {true},
  author =    {Qiao, Yuchuan and Lelieveldt, Boudewijn P.F. and Staring, Marius},
  title =     {Fast automatic estimation of the optimization step size for nonrigid image registration},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Ourselin, Sebastien and Styner, Martin A.},
  address =   {San Diego, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {9034},
  pages =     {90341A},
  month =     {February},
  year =      {2014},
  pdf =       {2014_c_SPIEMI.pdf},
  html =      {http://dx.doi.org/10.1117/12.2042859},
  arxiv =     {},
  code =      {},
  abstract =  {Image registration is often used in the clinic, for example during radiotherapy and image-guide surgery, but also for general image analysis. Currently, this process is often very slow, yet for intra-operative procedures the speed is crucial. For intensity-based image registration, a nonlinear optimization problem should be solved, usually by (stochastic) gradient descent. This procedure relies on a proper setting of a parameter which controls the optimization step size. This parameter is difficult to choose manually however, since it depends on the input data, optimization metric and transformation model. Previously, the Adaptive Stochastic Gradient Descent (ASGD) method has been proposed that automatically chooses the step size, but it comes at high computational cost. In this paper, we propose a new computationally efficient method to automatically determine the step size, by considering the observed distribution of the voxel displacements between iterations. A relation between the step size and the expectation and variance of the observed distribution is then derived. Experiments have been performed on 3D lung CT data (19 patients) using a nonrigid B-spline transformation model. For all tested dissimilarity metrics (mean squared distance, normalized correlation, mutual information, normalized mutual information), we obtained similar accuracy as ASGD. Compared to ASGD whose estimation time is progressively increasing with the number of parameters, the estimation time of the proposed method is substantially reduced to an almost constant time, from 40 seconds to no more than 1 second when the number of parameters is 10<sup>5</sup>.},
}

@inproceedings{Dzyubachyk:2013,
  abbr =    {},
  bibtex_show = {true},
  author =    {Dzyubachyk, Oleh and van der Geest, Rob J. and Staring, Marius and B{\"o}rnert, Peter and Reijnierse, Monique and Bloem, Johan L. and Lelieveldt, Boudewijn P.F.},
  title =     {Joint Intensity Inhomogeneity Correction for Whole-Body MR Data},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  editor =    {Mori, K. and Sakuma, I. and Sato, Y. and Barillot, C. and Navab, N.},
  address =   {Nagoya, Japan},
  series =    {Lecture Notes in Computer Science},
  volume =    {8149},
  pages =     {106 - 113},
  month =     {September},
  year =      {2013},
  pdf =       {2013_c_MICCAI.pdf},
  html =      {http://dx.doi.org/10.1007/978-3-642-40811-3_14},
  arxiv =     {},
  code =      {},
  abstract =  {Whole-body MR receives increasing interest as potential alternative to many conventional diagnostic methods. Typical whole-body MR scans contain multiple data channels and are acquired in a multistation manner. Quantification of such data typically requires correction of two types of artefacts: different intensity scaling on each acquired image stack, and intensity inhomogeneity (bias) within each stack. In this work, we present an all-in-one method that is able to correct for both mentioned types of acquisition artefacts. The most important properties of our method are: 1) All the processing is performed jointly on all available data channels, which is necessary for preserving the relation between them, and 2) It allows easy incorporation of additional knowledge for estimation of the bias field. Performed validation on two types of whole-body MR data confirmed superior performance of our approach in comparison with state-of-the-art bias removal methods.},
}

@inproceedings{Xiao:2013,
  abbr =    {},
  bibtex_show = {true},
  author =    {Xiao, Changyan and Staring, Marius and Wang, Juan and Shamonin, Denis P. and Stoel, Berend C.},
  title =     {A Derivative of Stick Filter for Pulmonary Fissure Detection in CT Images},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Ourselin, S. and Haynor, D.R.},
  address =   {Orlando, Florida, USA},
  series =    {Proceedings of SPIE},
  volume =    {8669},
  pages =     {86690V},
  month =     {February},
  year =      {2013},
  pdf =       {2013_c_SPIEMIb.pdf},
  html =      {http://dx.doi.org/10.1117/12.2006566},
  arxiv =     {},
  code =      {},
  abstract =  {Pulmonary fissures are important landmarks for automated recognition of lung anatomy. We propose a derivative of stick (DoS) filter for fissure detection in CT scans by considering the thin linear shape across multiple transverse planes. Based on a stick decomposition of a rectangle neighborhood, our main contribution is to define a nonlinear derivative vertical to the stick orientation. Then, combining with a standard deviation of intensity along the stick, the composed likelihood function will take a strong response to fissure-like bright lines, and tends to suppress undesired structures including large vessels, step edges and blobs. Applying the 2D filter sequentially to the sagittal, coronal and axial planes, an approximate 3D co-planar constraint is implicitly exerted through the cascaded pipeline, which helps to further remove the non-fissure tissues. To generate a clear segmentation, we adopt a connected component based post-processing scheme, and a branch-point finding algorithm is introduced to disconnect the residual adjacent clutters from the fissures, after binarizing the filter response with a relatively low threshold. The performance of our filter has been verified in experiments with a 23 patients dataset, where pathological deformations to different extents are included. It compared favorably with prior algorithms.},
}

@inproceedings{Khmelinskii:2013,
  abbr =    {},
  bibtex_show = {true},
  author =    {Khmelinskii, A. and Mengler, L. and Kitslaar, P. and Staring, M. and Hoehn, M. and Lelieveldt, B.P.F.},
  title =     {A visualization platform for high-throughput, follow-up, co-registered multi-contrast MRI rat brain data},
  booktitle = {SPIE Medical Imaging: Biomedical Applications in Molecular, Structural, and Functional Imaging},
  editor =    {Weaver, J.B. and Molthen, R.C.},
  address =   {Orlando, Florida, USA},
  series =    {Proceedings of SPIE},
  volume =    {8672},
  pages =     {86721W},
  month =     {February},
  year =      {2013},
  pdf =       {2013_c_SPIEMIa.pdf},
  html =      {http://dx.doi.org/10.1117/12.2006529},
  arxiv =     {},
  code =      {},
  abstract =  {Multi-contrast MRI is a frequently used imaging technique in preclinical brain imaging. In longitudinal cross-sectional studies exploiting and browsing through this high-throughput, heterogeneous data can become a very demanding task. The goal of this work was to build an intuitive and easy to use, dedicated visualization and side-by-side exploration tool for heterogeneous, co-registered multi-contrast, follow-up cross-sectional MRI data. The registration by-products were used: the deformation field was used to automatically link the same voxel in the displayed datasets of interest. Its determinant of the Jacobian (detJac) was used for a faster and more accurate visual assessment and comparison of brain deformation between the follow-up scans. This was combined with an efficient data management scheme. We investigated the functionality and the utility of our tool in the neuroimaging research field by means of a case study evaluation with three experienced domain scientists, using longitudinal, cross-sectional multi-contrast MRI rat brain data. Based on the performed case study evaluation we can conclude that the proposed tool improves the visual assessment of high-throughput cross-sectional, multi-contrast, follow-up data and can further assist in guiding quantitative studies.},
}

@inproceedings{Agarwal:2012,
  abbr =    {},
  bibtex_show = {true},
  author =    {Agarwal, Maruti and Bakker, M. Els and Hendriks, Emile A. and Stoel, Berend C. and Reiber, Johan H.C. and Staring, Marius},
  title =     {Local SIMPLE Multi Atlas-Based Segmentation Applied to Lung Lobe Detection on Chest CT},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Haynor, D.R. and Ourselin, S.},
  address =   {San Diego, California, USA},
  series =    {Proceedings of SPIE},
  volume =    {8314},
  pages =     {831410},
  month =     {February},
  year =      {2012},
  pdf =       {2012_c_SPIEMIb.pdf},
  html =      {http://dx.doi.org/10.1117/12.911552},
  arxiv =     {},
  code =      {},
  abstract =  {For multi atlas-based segmentation approaches, a segmentation fusion scheme which considers local performance measures may be more accurate than a method which uses a global performance measure. We improve upon an existing segmentation fusion method called SIMPLE and extend it to be localized and suitable for multi-labeled segmentations. We demonstrate the algorithm performance on 23 CT scans of COPD patients using a leaveone-out experiment. Our algorithm performs significantly better (p < 0.01) than majority voting, STAPLE, and SIMPLE, with a median overlap of the fissure of 0.45, 0.48, 0.55 and 0.6 for majority voting, STAPLE, SIMPLE, and the proposed algorithm, respectively.},
}

@inproceedings{Shamonin:2012,
  abbr =    {},
  bibtex_show = {true},
  author =    {Shamonin, Denis P. and Staring, Marius and Bakker, M. Els and Xiao, Changyan and Stolk, Jan and Reiber, Johan H.C. and Stoel, Berend C.},
  title =     {Automatic Lung Lobe Segmentation of COPD Patients using Iterative B-Spline Fitting},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Haynor, D.R. and Ourselin, S.},
  address =   {San Diego, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {8314},
  pages =     {83140W},
  month =     {February},
  year =      {2012},
  pdf =       {2012_c_SPIEMIa.pdf},
  html =      {http://dx.doi.org/10.1117/12.910869},
  arxiv =     {},
  code =      {},
  abstract =  {We present an automatic lung lobe segmentation algorithm for COPD patients. The method enhances fissures, removes unlikely fissure candidates, after which a B-spline is fitted iteratively through the remaining candidate objects. The iterative fitting approach circumvents the need to classify each object as being part of the fissure or being noise, and allows the fissure to be detected in multiple disconnected parts. This property is beneficial for good performance in patient data, containing incomplete and disease-affected fissures.<br>The proposed algorithm is tested on 22 COPD patients, resulting in accurate lobe-based densitometry, and a median overlap of the fissure (defined 3 voxels wide) with an expert ground truth of 0.65, 0.54 and 0.44 for the three main fissures. This compares to complete lobe overlaps of 0.99, 0.98, 0.98, 0.97 and 0.87 for the five main lobes, showing promise for lobe segmentation on data of patients with moderate to severe COPD.},
}

@inproceedings{Liang:2011,
  abbr =    {},
  bibtex_show = {true},
  author =    {Liang, Xi and Kotagiri, R. and Yang, Q. and Staring, Marius and Pitman, A.},
  title =     {Generating Coefficients for Regularization Terms in Nonrigid Registration of Contrast-Enhanced MRI},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention, Workshop on Breast Image Analysis},
  editor =    {Tanner, C. and Schnabel, J. and Karssemeijer, N. and Nielsen, Mads and Giger, M. and Hawkes, D.J.},
  address =   {Toronto, Canada},
  pages =     {1 - 8},
  month =     {September},
  year =      {2011},
  pdf =       {2011_c_MICCAIc.pdf},
  html =      {},
  arxiv =     {},
  code =      {},
  abstract =  {Nonrigid registration is a technique to recover spatial deformations between images. It can be formulated as an optimization problem to minimize the image dissimilarity. A regularization term is used to reduce undesirable deformations which are usually employed in a homogeneous or spatial-variant fashion. When spatial-variant regularization is used in nonrigid registration of dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI), the local coefficients have been determined by manual segmentation of tissues of interest. We propose a framework to generate regularization coefficients for nonrigid registration in DCE-MRI, where tumor locations are to be transformed in a rigid fashion. The coefficients are obtained by applying a sigmoid function on subtraction images from a pre-registration. All parameters in the function are automatically determined using k-means clustering. The validation study compares three regularization weighting schemes in nonrigid registrations: a constant coefficient for a volume-preserving term, binary coefficients obtained by manual segmentation and a real-value coefficients using the proposed method on a rigidity term. Evaluation is performed using displacements, intensity changes and volume changes of tumors on synthetic and clinical DCE-MR breast images. As a result, the registration using spatial-variant rigidity terms performs better than using homogeneous volume-preserving terms. For the coefficient generation methods of a rigidity term, the proposed method can replace the binary coefficients requiring manual tumor segmentation.},
}

@inproceedings{Baiker:2011,
  abbr =    {},
  bibtex_show = {true},
  author =    {Baiker, Martin and Staring, Marius and L{\"o}wik, Clemens W.G.M. and Reiber, Johan H.C. and Lelieveldt, Boudewijn P.F.},
  title =     {Automated Registration of Whole-Body Follow-Up MicroCT Data of Mice},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  editor =    {Fichtinger, G. and Martel, A. and Peters, T.},
  address =   {Toronto, Canada},
  series =    {Lecture Notes in Computer Science},
  volume =    {6892},
  pages =     {516 - 523},
  month =     {September},
  year =      {2011},
  pdf =       {2011_c_MICCAIb.pdf},
  html =      {http://dx.doi.org/10.1007/978-3-642-23629-7_63},
  arxiv =     {},
  code =      {https://github.com/SuperElastix/elastix},
  abstract =  {In vivo MicroCT imaging of disease models at multiple time points is of great importance for preclinical oncological research, to monitor disease progression. However, the great postural variability between animals in the imaging device complicates data comparison.<br>In this paper we propose a method for automated registration of whole-body MicroCT follow-up datasets of mice. First, we register the skeleton, the lungs and the skin of an articulated animal atlas (Segars et al. 2004) to MicroCT datasets, yielding point correspondence of these structures over all time points. This correspondence is then used to regularize an intensity-based B-spline registration. This two step approach combines the robustness of model-based registration with the high accuracy of intensity-based registration.<br>We demonstrate our approach using challenging whole-body in vivo follow-up MicroCT data and obtain subvoxel accuracy for the skeleton and the skin, based on the Euclidean surface distance. The method is computationally efficient and enables high resolution whole-body registration in &asymp;17 minutes with unoptimized code, mostly executed single-threaded.},
}

@inproceedings{Klein:2011,
  abbr =    {},
  bibtex_show = {true},
  author =    {Klein, Stefan and Staring, Marius and Andersson, Patrik and Pluim, Josien P.W.},
  title =     {Preconditioned Stochastic Gradient Descent Optimisation for Monomodal Image Registration},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  editor =    {Fichtinger, G. and Martel, A. and Peters, T.},
  address =   {Toronto, Canada},
  series =    {Lecture Notes in Computer Science},
  volume =    {6892},
  pages =     {549 - 556},
  month =     {September},
  year =      {2011},
  pdf =       {2011_c_MICCAIa.pdf},
  html =      {http://dx.doi.org/10.1007/978-3-642-23629-7_67},
  arxiv =     {},
  code =      {},
  abstract =  {We present a stochastic optimisation method for intensity-based monomodal image registration. The method is based on a Robbins-Monro stochastic gradient descent method with adaptive step size estimation, and adds a preconditioning matrix. The derivation of the preconditioner is based on the observation that, after registration, the deformed moving image should approximately equal the fixed image. This prior knowledge allows us to approximate the Hessian at the minimum of the registration cost function, without knowing the coordinate transformation that corresponds to this minimum. The method is validated on 3D fMRI time-series and 3D CT chest follow-up scans. The experimental results show that the preconditioning strategy improves the rate of convergence.},
}

@inproceedings{vanderBom:2011,
  abbr =    {},
  bibtex_show = {true},
  author =    {van der Bom, Martijn J. and Klein, Stefan and Staring, Marius and Homan, R. and Bartels, L. Wilbert and Pluim, Josien P.W.},
  title =     {Evaluation of optimization methods for intensity-based 2D-3D registration in x-ray guided interventions},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Dawant, B.M. and Haynor, D.R.},
  address =   {Orlando, Florida, USA},
  series =    {Proceedings of SPIE},
  volume =    {7962},
  pages =     {796223},
  month =     {February},
  year =      {2011},
  pdf =       {2011_c_SPIEMI.pdf},
  html =      {http://dx.doi.org/10.1117/12.877655},
  arxiv =     {},
  code =      {https://github.com/SuperElastix/elastix},
  abstract =  {The advantage of 2D-3D image registration methods versus direct image-to-patient registration, is that these methods generally do not require user interaction (such as manual annotations), additional machinery or additional acquisition of 3D data.<br>A variety of intensity-based similarity measures has been proposed and evaluated for different applications. These studies showed that the registration accuracy and capture range are influenced by the choice of similarity measure. However, the influence of the optimization method on intensity-based 2D-3D image registration has not been investigated. We have compared the registration performance of seven optimization methods in combination with three similarity measures: gradient difference, gradient correlation, and pattern intensity. Optimization methods included in this study were: regular step gradient descent, Nelder-Mead, Powell-Brent, Quasi-Newton, nonlinear conjugate gradient, simultaneous perturbation stochastic approximation, and evolution strategy. Registration experiments were performed on multiple patient data sets that were obtained during cerebral interventions. Various component combinations were evaluated on registration accuracy, capture range, and registration time. The results showed that for the same similarity measure, different registration accuracies and capture ranges were obtained when different optimization methods were used. For gradient difference, largest capture ranges were obtained with Powell-Brent and simultaneous perturbation stochastic approximation. Gradient correlation and pattern intensity had the largest capture ranges in combination with Powell-Brent, Nelder-Mead, nonlinear conjugate gradient, and Quasi-Newton. Average registration time, expressed in the number of DRRs required for convergence, was the lowest for Powell-Brent. Based on these results, we conclude that Powell-Brent is a reliable optimization method for intensity-based 2D-3D registration of x-ray images to CBCT, regardless of the similarity measure used.},
}

@inproceedings{Farago:2010,
  abbr =    {},
  bibtex_show = {true},
  author =    {Farago, Tamas and Nikolov, Hristo and Klein, Stefan and Reiber, Johan H.C. and Staring, Marius},
  title =     {Semi-Automatic Parallelisation for Iterative Image Registration with B-splines},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention, High Performance workshop},
  editor =    {Gong, Leiguang and etal,},
  address =   {Beijing, China},
  month =     {September},
  year =      {2010},
pdf =       {2010_c_MICCAIc.pdf},
  html =      {},
  arxiv =     {},
  code =      {},
  abstract =  {Nonrigid image registration is an important, but resource demanding and time-consuming task in medical image analysis. This limits its application in time-critical clinical routines. In this paper we explore acceleration of a registration algorithm by means of parallel processing. The serial algorithm is analysed and automatically rewritten (re-coded) by a recently introduced automatic parallelisation tool, <span class="sc">Daedalus</span>. <span class="sc">Daedalus</span> identifies task parallelism (which is more difficult than data parallelism) and converts the serial algorithm to a Polyhedral Process Network (PPN). Each process node in the PPN corresponds to a task that is mapped to a separate thread (of the CPU, but possibly also GPU). The threads communicate via first-in-first-out (FIFO) buffers. Difficulties such as deadlocks, race conditions and synchronisation issues are automatically taken care of by <span class="sc">Daedalus</span>. Data-parallelism is not automatically recognised by <span class="sc">Daedalus</span>, but can be achieved by manually prefactoring the serial code to make data parallelism explicit. We evaluated the performance gain on a 4-core CPU and compared it to an OpenMP implementation, exploiting only data parallelism. A speedup factor of 3.4 was realised using <span class="sc">Daedalus</span>, versus 2.6 using OpenMP. The automated <span class="sc">Daedalus</span> approach seems thus a promising means of accelerating image registration based on task parallelisation.},
}

@inproceedings{Staring:2010,
  abbr =    {},
  bibtex_show = {true},
  author =    {Staring, Marius and Klein, Stefan and Reiber, Johan H.C. and Niessen, Wiro J. and Stoel, Berend C.},
  title =     {Pulmonary Image Registration With elastix Using a Standard Intensity-Based Algorithm},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention, Medical Image Analysis for the Clinic: A Grand Challenge},
  editor =    {van Ginneken, Bram and Murphy, Keelin and Heimann, Tobias and Pekar, Vladimir and Deng, Xiang},
  address =   {Beijing, China},
  month =     {September},
  year =      {2010},
  pdf =       {2010_c_MICCAIb.pdf},
  html =      {},
  arxiv =     {},
  code =      {https://github.com/SuperElastix/elastix},
  abstract =  {Accurate registration of thoracic CT is useful in clinical terms and also challenging due to the elastic nature of lung tissue deformations. The goal of the EMPIRE10 challenge (Evaluation of Methods for Pulmonary Image Registration 2010), a workshop of the MICCAI 2010 conference, is to provide a platform for in-depth evaluation and fair comparison of available registration algorithms for this application. To this end we registered to the challenge with team RubberBand. The goal of our submission is to determine what a standard, but fully automatic, intensity-based image registration algorithm can achieve compared to the competition.<br>The algorithm, implemented in <code>elastix</code>, optimises the normalised correlation criterion, using a fast, parameter-free and robust stochastic optimisation procedure. A combination of an affine and two nonrigid B-spline transformations models the spatial relationship. The approach is embedded in a multi-resolution framework for both the image data and the transformation. No explicit regularisation is used.<br>Of the 34 submitted algorithms, our contribution achieved the 7-th place with an average rank of 13.13 (best 8.03, worst 31.46). The incorporation of a regularisation term may improve the ranking of the algorithm, since our final score was most negatively influenced by the score for folding.},
}

@inproceedings{Xiao:2010,
  abbr =    {},
  bibtex_show = {true},
  author =    {Xiao, Changyan and Staring, Marius and Shamonin, Denis P. and Reiber, Johan H.C. and Stolk, Jan and Stoel, Berend C.},
  title =     {A Strain Energy Filter for 3D Vessel Enhancement},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  editor =    {T. Jiang, and etal,},
  address =   {Beijing, China},
  series =    {Lecture Notes in Computer Science},
  volume =    {6363},
  pages =     {367 - 374},
  year =      {2010},
  pdf =       {2010_c_MICCAIa.pdf},
  html =      {http://dx.doi.org/10.1007/978-3-642-15711-0_46},
  arxiv =     {},
  code =      {https://github.com/ITKTools/ITKTools},
  abstract =  {The traditional Hessian-related vessel filters often suffer from the problem of handling non-cylindrical objects. To remedy the shortcoming, we present a shape-tuned strain energy density function to measure vessel likelihood in 3D images. Based on the tensor invariants and stress-strain principle in mechanics, a new shape discriminating and vessel strength measure function is formulated. The synthetical and clinical data experiments verify the performance of our method in enhancing complex vascular structures including branches, bifurcations, and feature details.},
}

@inproceedings{Scheenstra:2009,
  abbr =    {},
  bibtex_show = {true},
  author =    {Scheenstra, Alize E.H. and Muskulus, M. and Staring, Marius and van den Maagdenberg, A.M.J.V. and Verduyn Lunel, S. and Reiber, Johan H.C. and van der Weerd, L. and Dijkstra, Jouke},
  title =     {The 3D Moore-Rayleigh Test for the Quantitative Groupwise Comparison of MR Brain Images},
  booktitle = {Information Processing in Medical Imaging},
  editor =    {Prince, J. L. and Pham, D.L. and Myers, K.J.},
  address =   {Williamsburg, Virginia, USA},
  series =    {Lecture Notes in Computer Science},
  volume =    {5636},
  pages =     {564 - 575},
  month =     {July},
  year =      {2009},
  pdf =       {2009_c_IPMI.pdf},
  html =      {http://dx.doi.org/10.1007/978-3-642-02498-6_47},
  arxiv =     {},
  code =      {},
  abstract =  {Non-rigid registration of MR images to a common reference image results in deformation fields, from which anatomical differences can be statistically assessed, within and between populations. Without further assumptions, nonparametric tests are required and currently the analysis of deformation fields is performed by permutation tests. For deformation fields, often the vector magnitude is chosen as test statistic, resulting in a loss of information. In this paper, we consider the three dimensional Moore-Rayleigh test as an alternative for permutation tests. This nonparametric test offers two novel features: first, it incorporates both the directions and magnitude of the deformation vectors. Second, as its distribution function is available in closed form, this test statistic can be used in a clinical setting. Using synthetic data that represents variations as commonly encountered in clinical data, we show that the Moore-Rayleigh test outperforms the classical permutation test.},
}

@inproceedings{Staring:2009,
  abbr =    {},
  bibtex_show = {true},
  author =    {Staring, Marius and Bakker, M. Els and Shamonin, Denis P. and Stolk, Jan and Reiber, Johan H.C. and Stoel, Berend C.},
  title =     {Towards Local Estimation of Emphysema Progression Using Image Registration},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Pluim, J.P.W. and Dawant, B.M.},
  address =   {Orlando, Florida, USA},
  series =    {Proceedings of SPIE},
  volume =    {7259},
  pages =     {72590O},
  month =     {February},
  year =      {2009},
  pdf =       {2009_c_SPIEMI.pdf},
  html =      {http://dx.doi.org/10.1117/12.811576},
  arxiv =     {},
  code =      {},
  abstract =  {Progression measurement of emphysema is required to evaluate the health condition of a patient and the effect of drugs. To locally estimate progression we use image registration, which allows for volume correction using the determinant of the Jacobian of the transformation. We introduce an adaptation of the so-called sponge model that circumvents its constant-mass assumption. Preliminary results from CT scans of a lung phantom and from CT data sets of two patients suggest that image registration may be a suitable method to locally estimate emphysema progression.},
}

@inproceedings{Murphy:2008b,
  abbr =    {},
  bibtex_show = {true},
  author =    {Murphy, Keelin and van Ginneken, Bram and Pluim, Josien P.W. and Klein, Stefan and Staring, Marius},
  title =     {Semi-automatic Reference Standard Construction for Quantitative Evaluation of Lung CT Registration},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention},
  editor =    {Fichtinger, G. and Martel, A. and Peters, T.},
  series =    {Lecture Notes in Computer Science},
  volume =    {5242},
  pages =     {1006 - 1013},
  month =     {September},
  year =      {2008},
  pdf =       {2008_c_MICCAI.pdf},
  html =      {http://dx.doi.org/10.1007/978-3-540-85990-1_121},
  arxiv =     {},
  code =      {},
  abstract =  {An algorithm is presented for the efficient semi-automatic construction of a detailed reference standard for registration in thoracic CT. A well-distributed set of 100 landmarks is detected fully automatically in one scan of a pair to be registered. Using a custom-designed interface, observers locate corresponding anatomic locations in the second scan. The manual annotations are used to learn the relationship between the scans and after approximately twenty manual marks the remaining points are matched automatically. Inter-observer differences demonstrate the accuracy of the matching and the applicability of the reference standard is demonstrated on two different sets of registration results over 19 CT scan pairs.},
}

@inproceedings{Murphy:2008a,
  abbr =    {},
  bibtex_show = {true},
  author =    {Murphy, Keelin and van Ginneken, Bram and Pluim, Josien P.W. and Klein, Stefan and Staring, Marius},
  title =     {Quantitative Assessment of Registration in Thoracic CT},
  booktitle = {The First International Workshop on Pulmonary Image Analysis},
  editor =    {Brown, Matthew and de Bruijne, Marleen and van Ginneken, Bram and Kiraly, Atilla and Kuhnigk, Jan Martin and Lorenz, Cristian and Mori, Kensaku and Reinhardt, Joseph},
  address =   {New York, USA},
  pages =     {203 - 211},
  month =     {September},
  year =      {2008},
  pdf =       {2008_c_PIA.pdf},
  html =      {http://www.lulu.com/content/3507981},
  arxiv =     {},
  code =      {},
  abstract =  {A novel method for quantitative evaluation of registration systems in thoracic CT is utilised to examine the effects of varying system parameters on registration error. Regional analysis is implemented to determine whether registration error is more prevalent in particular areas of the lungs. Experiments on twenty-four CT scan-pairs prove that in many cases significant reductions in processing time can be achieved without much loss of registration accuracy. More difficult cases require additional steps in order to achieve maximum precision. Larger errors appear more frequently in the lower regions of the lungs close to the diaphragm.},
}

@inproceedings{vanRikxoort:2008,
  abbr =    {},
  bibtex_show = {true},
  author =    {van Rikxoort, Eva and I{\v{s}}gum, Ivana and Staring, Marius and Klein, Stefan and van Ginneken, Bram},
  title =     {Adaptive local multi-atlas segmentation: application to heart segmentation in chest CT scans},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Reinhardt, J.M. and Pluim, J.P.W.},
  address =   {San Diego, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {6914},
  pages =     {691407},
  month =     {February},
  year =      {2008},
  pdf =       {2008_c_SPIEMI.pdf},
  html =      {http://dx.doi.org/10.1117/12.772301},
  arxiv =     {},
  code =      {},
  abstract =  {Atlas-based segmentation is a popular generic technique for automated delineation of structures in volumetric data sets. Several studies have shown that multi-atlas based segmentation methods outperform schemes that use only a single atlas, but running multiple registrations on large volumetric data is too time-consuming for routine clinical use. We propose a generally applicable adaptive local multi-atlas segmentation method (ALMAS) that locally decides how many and which atlases are needed to segment a target image. Only the selected parts of atlases are registered. The method is iterative and automatically stops when no further improvement is expected. ALMAS was applied to segmentation of the heart on chest CT scans and compared to three existing atlas-based methods. It performed significantly better than single-atlas methods and as good as multi-atlas methods at a much lower computational cost.},
}

@inproceedings{Klein:2007b,
  abbr =    {},
  bibtex_show = {true},
  author =    {Klein, Stefan and van der Heide, Uulke A. and Staring, Marius and Kotte, Alexis N.T.J. and Raaymakers, Bas W. and Pluim, Josien P.W.},
  title =     {Segmentation of the Prostate in MR images by Atlas Matching using Localised Mutual Information},
  booktitle = {XVth International Conference on the use of Computers in Radiation Therapy},
  editor =    {Jaffray, D.A. and Sharpe, M. and van Dyk, J. and Bissonnette, J.P.},
  address =   {Toronto, Canada},
  volume =    {2},
  pages =     {585 - 589},
  month =     {June},
  year =      {2007},
  pdf =       {2007_c_ICCR.pdf},
  html =      {},
  arxiv =     {},
  code =      {},
  abstract =  {In this paper, an automatic method for delineating the prostate in MR scans is presented. The method is based on nonrigid registration of a set of prelabelled atlas images. Each atlas image is nonrigidly registered with the target patient image. After that, atlas images that match well to the patient image are selected and the segmentation is obtained by a majority voting rule. Two registration methods are investigated. The first one uses the common mutual information as a similarity measure. The second one uses a localised version of mutual information. Experiments are performed on 38 MR images using a leave-one-out approach. The automatic segmentations are evaluated with manual segmentations by computing their overlap. The localised mutual information measure outperforms the commonly used global version and achieves a median Dice similarity coefficient of 0.82. The spatial distribution of the segmentation errors is visualised using a spherical coordinate mapping of the prostate boundary.},
}

@inproceedings{Klein:2007a,
  abbr =    {},
  bibtex_show = {true},
  author =    {Klein, Stefan and van der Heide, Uulke A. and Raaymakers, Bas W. and Kotte, Alexis N.T.J. and Staring, Marius and Pluim, Josien P.W.},
  title =     {Segmentation of the prostate in MR images by atlas matching},
  booktitle = {IEEE International Symposium on Biomedical Imaging (ISBI)},
  editor =    {Fessler, J.A. and Denney Jr., T.S.},
  address =   {Washington, USA},
  pages =     {1300 - 1303},
  month =     {April},
  year =      {2007},
  pdf =       {2007_c_ISBI.pdf},
  html =      {http://dx.doi.org/10.1109/ISBI.2007.357098},
  arxiv =     {},
  code =      {},
  abstract =  {Prostate cancer treatment by radiation therapy requires an accurate localisation of the prostate. For the treatment planning, primarily computed tomography (CT) images are used, but increasingly magnetic resonance (MR) images are added, because of their soft-tissue contrast. In current practice at our hospital, a manual delineation of the prostate is made, based on the CT and MR scans, which is a labour-intensive task. We propose an automatic segmentation method, based on nonrigid registration of a set of prelabelled MR atlas images. The algorithm consists of three stages. Firstly, the target image is nonrigidly registered with each atlas image, using mutual information as the similarity measure. After that, the best registered atlas images are selected by comparing the mutual information values after registration. Finally, the segmentation is obtained by averaging the selected deformed segmentations and thresholding the result. The method is evaluated on 22 images by calculating the overlap of automatic  and manual segmentations. This results in a median Dice similarity coefficient of 0.82.},
}

@inproceedings{Staring:2006b,
  abbr =    {},
  bibtex_show = {true},
  author =    {Staring, Marius and Klein, Stefan and Pluim, Josien P.W.},
  title =     {Evaluation of a Rigidity Penalty Term for Nonrigid Registration},
  booktitle = {Workshop on Image Registration in Deformable Environments},
  editor =    {Bartoli, Adrien and Navab, Nassir and Lepetit, Vincent},
  address =   {Edinburgh, UK},
  pages =     {41 - 50},
  month =     {September},
  year =      {2006},
  pdf =       {2006_c_Deform.pdf},
  html =      {},
  arxiv =     {},
  code =      {},
  abstract =  {Nonrigid registration of medical images usually does not model properties of different tissue types. This results for example in nonrigid deformations of structures that are rigid. In this work we address this problem by employing a local rigidity penalty term. We illustrate this approach on a 2D synthetic image, and evaluate it on clinical 2D DSA image sequences, and on 3D CT follow-up data of the thorax of patients suffering from lung tumours. The results show that the rigidity penalty term does indeed penalise nonrigid deformations of rigid structures, whereas the standard nonrigid registration algorithm compresses those.},
}

@inproceedings{Klein:2006,
  abbr =    {},
  bibtex_show = {true},
  author =    {Klein, Stefan and Staring, Marius and Pluim, Josien P.W.},
  title =     {A Comparison of Acceleration Techniques for Nonrigid Medical Image Registration},
  booktitle = {International Workshop on Biomedical Image Registration (WBIR)},
  editor =    {Pluim, J.P.W. and Likar, B. and Gerritsen, F.A.},
  address =   {Utrecht, The Netherlands},
  series =    {Lecture Notes on Computer Science},
  volume =    {4057},
  pages =     {151 - 159},
  month =     {July},
  year =      {2006},
  pdf =       {2006_c_WBIR.pdf},
  html =      {http://dx.doi.org/10.1007/11784012_19},
  arxiv =     {},
  code =      {},
  abstract =  {Mutual information based nonrigid registration of medical images is a popular approach. The coordinate mapping that relates the two images is found in an iterative optimisation procedure. In every iteration a computationally expensive evaluation of the mutual information's derivative is required. In this work two acceleration strategies are compared. The first technique aims at reducing the number of iterations, and, consequently, the number of derivative evaluations. The second technique reduces the computational costs <i>per iteration</i> by employing stochastic approximations of the derivatives. The performance of both methods is tested on an artificial registration problem, where the ground truth is known, and on a clinical problem involving low-dose CT scans and large deformations. The experiments show that the stochastic approximation approach is superior in terms of speed and robustness. However, more accurate solutions are obtained with the first technique.},
}

@inproceedings{Staring:2006a,
  abbr =    {},
  bibtex_show = {true},
  author =    {Staring, Marius and Klein, Stefan and Pluim, Josien P.W.},
  title =     {Nonrigid Registration Using a Rigidity Constraint},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Reinhardt, J.M. and Pluim, J.P.W.},
  address =   {San Diego, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {6144},
  pages =     {355 - 364},
  month =     {February},
  year =      {2006},
  pdf =       {2006_c_SPIEMI.pdf},
  html =      {http://dx.doi.org/10.1117/12.654362},
  arxiv =     {},
  code =      {},
  abstract =  {Nonrigid registration is a technique commonly used in the field of medical imaging. A drawback of most current nonrigid registration algorithms is that they model all tissue as being nonrigid. When a nonrigid registration is performed, the rigid objects in the image, such as bony structures or surgical instruments, may also transform nonrigidly. Other consequences are that tumour growth between follow-up images may be concealed, or that structures containing contrast material in one image and not in the other may be compressed by the registration algorithm.<br>In this paper we propose a novel regularisation term, which is added to the cost function in order to penalise nonrigid deformations of rigid objects. This regularisation term can be used for any representation of the deformation field capable of modelling locally rigid deformations. By using a B-spline representation of the deformation field, a fast algorithm can be devised. We show on 2D synthetic data, on clinical CT slices, and on clinical DSA images, that the proposed rigidity constraint is successful, thus improving registration results.},
}

@inproceedings{Noordmans:2006,
  abbr =    {},
  bibtex_show = {true},
  author =    {Noordmans, Herke Jan and de Roode, Rowland and Staring, Marius and Verdaasdonk, Rudolf},
  title =     {Registration and analysis of in-vivo multi-spectral images for correction of motion and comparison in time},
  booktitle = {SPIE Photonics West: Multimodal Biomedical Imaging},
  editor =    {Azar, F.S. and Metaxas, D.N.},
  address =   {San Jose, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {6081},
  pages =     {35 - 43},
  month =     {January},
  year =      {2006},
  pdf =       {2006_c_SPIEPW.pdf},
  html =      {http://dx.doi.org/10.1117/12.648085},
  arxiv =     {},
  code =      {},
  abstract =  {In-vivo image-based multi-spectral images have typical problems in image acquisition, registration, visualization and analysis. As its spatial and spectral axes do not have the same unit, standard image algorithms often do not apply. The image size is often so large that it is hard to analyze them interactively. In a clinical setting, image motion will always occur during the acquisition times up to 30 seconds, since most (elderly) patients often have difficulty to retain their poses. In this paper, we discuss how the acquisition, registration, display and analysis can be optimized for in-vivo multi-spectral images.},
}

@inproceedings{Staring:2005,
  abbr =    {},
  bibtex_show = {true},
  author =    {Staring, Marius and Klein, Stefan and Pluim, Josien P.W.},
  title =     {Nonrigid Registration with Adaptive, Content-Based Filtering of the Deformation Field},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Fitzpatrick, J.M. and Reinhardt, J.M.},
  address =   {San Diego, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {5747},
  pages =     {212 - 221},
  month =     {February},
  year =      {2005},
  pdf =       {2005_c_SPIEMIa.pdf},
  html =      {http://dx.doi.org/10.1117/12.594781},
  arxiv =     {},
  code =      {},
  abstract =  {In present-day medical practice it is often necessary to nonrigidly align image data, either intra- or inter-patient. Current registration algorithms usually do not take different tissue types into account. A problem that might occur with these algorithms is that rigid tissue, like bone, also deforms elastically. We propose a method to correct a deformation field, that is calculated with a nonrigid registration algorithm. The correction is based on a second feature image, which represents the tissue stiffness. The amount of smoothing of the deformation field is related to this stiffness coefficient. By filtering the deformation field on rigid tissue, the deformation field will represent a local rigid transformation. Other parts of the image containing nonrigid tissue are smoothed less, which leaves the original elastic deformation (almost) untouched. It is shown on a synthetic example and on inspiration-expiration CT data of the thorax, that a filtering of the deformation field based on tissue type indeed keeps rigid tissue rigid, thus improving the registration results.},
}

@inproceedings{Klein:2005,
  abbr =    {},
  bibtex_show = {true},
  author =    {Klein, Stefan and Staring, Marius and Pluim, Josien P.W.},
  title =     {Comparison of gradient approximation techniques for optimisation of mutual information in nonrigid registration},
  booktitle = {SPIE Medical Imaging: Image Processing},
  editor =    {Fitzpatrick, J.M. and Reinhardt, J.M.},
  address =   {San Diego, CA, USA},
  series =    {Proceedings of SPIE},
  volume =    {5747},
  pages =     {192 - 203},
  month =     {February},
  year =      {2005},
  pdf =       {2005_c_SPIEMIb.pdf},
  html =      {http://dx.doi.org/10.1117/12.595277},
  arxiv =     {},
  code =      {},
  abstract =  {Nonrigid registration of medical images by maximisation of their mutual information, in combination with a deformation field parameterised by cubic B-splines, has been shown to be robust and accurate in many applications. However, the high computation time is a big disadvantage. This work focusses on the optimisation procedure. Many implementations follow a gradient-descent like approach. The time needed for computing the derivative of the mutual information with respect to the B-spline parameters is the bottleneck in this process. We investigate the influence of several gradient approximation techniques on the number of iterations needed and the computation time per iteration. Three methods are studied: a simple finite difference strategy, the so-called simultaneous perturbation method, and a more analytic computation of the gradient based on a continuous, and differentiable representation of the joint histogram. In addition, the effect of decreasing the number of image samples, used for computing the gradient in each iteration, is investigated. Two types of experiments are performed. Firstly, the registration of an image to itself, after application of a known, randomly generated deformation, is considered. Secondly, experiments are performed with 3D ultrasound brain scans, and 3D CT follow-up scans of the chest. The experiments show that the method using an analytic gradient computation outperforms the other two. Furthermore, the computation time per iteration can be extremely decreased, without affecting the rate of convergence and final accuracy, by using very few samples of the image (randomly chosen every iteration) to compute the derivative. With this approach, large data sets (2563) can be registered within 5 minutes on a moderate PC.},
}

@inproceedings{Oostveen:2004,
  abbr =    {},
  bibtex_show = {true},
  author =    {Oostveen, Job C. and Kalker, A.A.C. and Staring, Marius},
  title =     {Adaptive quantization watermarking},
  booktitle = {Security, Steganography, and Watermarking of Multimedia Contents VI},
  editor =    {Delp III, Edward J. and Wong, Ping W.},
  address =   {San Jose, California, USA},
  series =    {Proceedings of SPIE},
  volume =    {5306},
  pages =     {296 - 303},
  month =     {January},
  year =      {2004},
  pdf =       {2004_c_SPIESSWMC.pdf},
  html =      {http://dx.doi.org/10.1117/12.526586},
  arxiv =     {},
  code =      {},
  abstract =  {In this paper we study the use of an adaptive quantization step size, instead of a fixed one, for the Scalar Costa Scheme. We propose an adaptation method based on Weber's law. This allows for a more effective embedding, which is also shown to render the watermark robust against sample value scaling. A model for the bit error probability due to the estimation of the adaptive quantization step size at the detector is derived, which provides insight in the required precision of estimating the quantization step size in the detector.},
}

@inproceedings{Staring:2003,
  abbr =    {},
  bibtex_show = {true},
  author =    {Staring, Marius and Oostveen, Job C. and Kalker, A.A.C.},
  title =     {Optimal distortion compensation for quantization watermarking},
  booktitle = {International Conference on Image Processing},
  address =   {Barcelona, Spain},
  volume =    {2},
  pages =     {727 - 730},
  month =     {September},
  year =      {2003},
  pdf =       {2003_c_ICIP.pdf},
  html =      {http://dx.doi.org/10.1109/ICIP.2003.1246783},
  arxiv =     {},
  code =      {},
  abstract =  {In this paper we study the problem of optimizing the distortion compensation parameter for the Scalar Costa Scheme, which is a practical version of the class of Distortion Compensated Dither Modulation schemes. In the literature, a number of results are known for finding the value of the distortion compensation parameter that maximizes the capacity of the watermarking channel. Instead, in this paper, we look at minimization of the bit error probability as the criterion for determining the optimal value of the distortion compensation parameter. To this end, we derive a model for the bit error probability, which is subsequently approximated and minimized. This is done both for the cases of Gaussian noise and uniform noise. The results match very well with earlier results by Eggers.},
}


