
@phdthesis{Jia:2024,
  abbr =    {},
  bibtex_show = {true},
  author =    {Jia, Jingnan},
  title =     {Automatic Analysis of Chest CT in Systemic Sclerosis Using Deep Learning},
  school =    {Leiden University Medical Center},
  address =   {Albinusdreef 2, 2333 ZA Leiden},
  month =     {September},
  year =      {2024},
  pdf =       {2024_t_Jia.pdf},
  html =      {},
  abstract =  {HRCT is an important modality to non-invasively diagnose pulmonary diseases and assess treatment effects. In this thesis, we developed automatic methods to quantify SSc disease, based on HRCT. In this thesis, we first provided a general introduction in Chapter 1 about pulmonary anatomy, SSc, PFTs, chest CT and deep learning on chest CT. A lung lobe segmentation method was proposed in Chapter 2, as accurately extracting lungs and lobes is an essential step for later SSc disease analysis. An explainable fully automated SSc-ILD scoring framework was proposed in Chapter 3. This framework could automatically select five levels and estimate the ratio of SSc-ILD to lung area for each level in the order of several seconds. In Chapter 4, an automatic PFT estimation network was developed which could help to understand the relation between lung function and structure and to estimate the PFTs from CT scans for patients with PFTs contraindications. Because of GPU memory limitation, the CT scans used in Chapter 4 were down-sampled. Therefore, Chapter 5 achieved higher PFT regression performance with less training time by converting vessel centerlines from HRCT to point cloud and graph data.},
}

@phdthesis{Elmahdy:2022,
  abbr =    {},
  bibtex_show = {true},
  author =    {Elmahdy, Mohamed S.},
  title =     {Deep learning for online adaptive radiotherapy},
  school =    {Leiden University Medical Center},
  address =   {Albinusdreef 2, 2333 ZA Leiden},
  month =     {March},
  year =      {2022},
  pdf =       {2022_t_Elmahdy.pdf},
  html =      {https://hdl.handle.net/1887/3278960},
  abstract =  {Adapting a radiotherapy treatment plan to the daily anatomy is a crucial task to ensure adequate irradiation of the target without unnecessary exposure of healthy tissue.This adaptation can be performed by automatically generating contours of the daily anatomy together with fast re-optimization of the treatment plan. These measurescan compensate for the daily variation and ensure the delivery of the prescribed dose distribution at small margins and high robustness settings. In this thesis, we focused on developing a deep learning-based methodology for automatic contouring for real-time adaptive radiotherapy either guided by CT or MR imaging modalities.},
}

@phdthesis{Sokooti:2021,
  abbr =    {},
  bibtex_show = {true},
  author =    {Sokooti, Hessam},
  title =     {Supervised Learning in Medical Image Registration},
  school =    {Leiden University Medical Center},
  address =   {Albinusdreef 2, 2333 ZA Leiden},
  month =     {December},
  year =      {2021},
  pdf =       {2021_t_Sokooti.pdf},
  html =      {https://hdl.handle.net/1887/3243762},
  abstract =  {The aim of this thesis is to develop a learning-based image registration method as a much faster alternative to conventional methods without requiring hyper-parameter tuning. We also aimed to improve accuracy as well as inference time of registration misalignment detection methods, via a fully automatic solution. Although all the proposed methods in this thesis are generic, all the experiments are performed on chest CT scans.<br>Chapter 2 presents a novel method to solve nonrigid image registration through a learning approach, instead of via iterative optimization of a predefined dissimilarity metric. Chapter 3 extends chapter 2 into a practical pipeline based on efficient supervised learning from artificial deformations. Chapter 4 proposes a new automatic method to predict the registration error in a quantitative manner and is applied to chest CT scans. Chapter 5 presents a supervised method to predict registration misalignment using convolutional neural networks (CNNs).},
}

@phdthesis{Zhai:2020,
  abbr =    {},
  bibtex_show = {true},
  author =    {Zhai, Zhiwei},
  title =     {Automatic Quantitative Analysis of Pulmonary Vessels in CT: Methods and Applications},
  school =    {Leiden University Medical Center},
  address =   {Albinusdreef 2, 2333 ZA Leiden},
  month =     {March},
  year =      {2020},
pdf =       {2020_t_Zhai.pdf},
  html =      {http://hdl.handle.net/1887/86281},
  abstract =  {The aim of this thesis is to develop these methods focusing on quantifying pulmonary vascular diseases and assessing treatment effects, based on CT images. Particularly, the following objectives have been pursued in this thesis: 1) to develop an accurate lung vessel segmentation method; 2) to propose and validate an automatic method for quantifying pulmonary vascular morphology; 3) to investigate pulmonary vascular remodeling in SSc patients with impaired DLCO, but in the absence of pulmonary fibrosis; 4) to investigate changes in the pulmonary vascular densitometry and morphology in patients with CTEPH, treated with BPA. These objectives are described in this thesis.},
}

@phdthesis{Qiao:2017,
  abbr =    {},
  bibtex_show = {true},
  author =    {Qiao, Yuchuan},
  title =     {Fast optimization methods for image registration in adaptive radiation therapy},
  school =    {Leiden University Medical Center},
  address =   {Albinusdreef 2, 2333 ZA Leiden},
  month =     {December},
  year =      {2017},
  pdf =       {2017_t_Qiao.pdf},
  html =      {http://hdl.handle.net/1887/59448},
  abstract =  {Image registration is important for medical image analysis. However, its clinical application is sometimes limited by the speed of the algorithm. For example, in online adaptive radiation therapy a few seconds is ideal, while it usually takes several minutes, at the least. In this thesis, we consider acceleration techniques for parametric intensity-based image registration problems focussing on the optimization routine, specifically the step size and the search direction. The different proposed methods are thoroughly evaluated on different datasets across modalities, subject, similarity measures and transformation models. Depending on the registration settings, the estimation time of the step size is reduced from 40 seconds to less than 1 second when the number of parameters is 105, almost 40 times faster. The total registration time of new acceleration techniques (FASGD) is reduced by a factor of 2.5-7x compared with ASGD for the experiments in this thesis. All methods were implemented using C++ in the open source registration package elastix. Based on these acceleration schemes we evaluated elastix on the application of automatic contour propagation in online adaptive intensity modulated proton therapy for prostate cancer.},
}

@phdthesis{Khmelinskii:2013,
  abbr =    {},
  bibtex_show = {true},
  author =    {Khmelinskii, Artem},
  title =     {Multi-modal small-animal imaging: image processing challenges and applications},
  school =    {Leiden University Medical Center},
  address =   {Albinusdreef 2, 2333 ZA Leiden},
  month =     {October},
  year =      {2013},
  pdf =       {2013_t_Khmelinskii.pdf},
  html =      {http://hdl.handle.net/1887/21914},
  abstract =  {In pre-clinical research, whole-body small animal imaging is widely used for the in vivo visualization of functional and anatomical information to study cancer, neurological and cardiovascular diseases and help with a faster development of new drugs. Functional information is provided by imaging modalities such as PET, SPECT and specialized MRI. Structural imaging modalities like radiography, CT, MRI and ultrasound provide detailed depictions of anatomy. Optical imaging modalities, such as BLI and near-infrared fluorescence imaging offer a high sensitivity in visualizing molecular processes in vivo. The combination of these modalities enables to follow the subject(s) and molecular processes in time, in living animals.<br>With these advances in image acquisition, the problem has shifted from data acquisition to data processing. The organization, analysis and interpretation of this heterogeneous whole-body imaging data has become a demanding task.<br>In this thesis, the data processing approach depicted in Figure 1.1 was further explored. This approach is based on an articulated whole-body atlas as a common reference to normalize the geometric heterogeneity caused by postural and anatomical differences between individuals and geometric differences between imaging modalities. Mapping to this articulated atlas has the advantage that all the different imaging modalities can be (semi) automatically registered to a common anatomical reference; postural variations can be corrected, and the different animals can be scaled properly while allowing for proper management of this highthroughput whole-body data.<br>In this thesis, we have focused on three complementary aspects of the approach described in Figure 1.1, and worked towards an automated analysis pipeline for quantitative small animal image analysis. The specific goals of this thesis were:<ol type="i"><li style="margin-bottom: 5px;">to further generalize the articulated atlas-based registration method to the multi-modality component of the global approach presented in Figure 1.1, focusing on SPECT and MRI whole-body mouse data</li><li style="margin-bottom: 5px;">to expand the Articulated Planar Reformation algorithm by linking it to recently introduced resolution-enhancing MR reconstruction techniques which enable "zooming in" on small anatomical details not detectable with conventional MRI</li><li>to prove the added value of atlas-based analysis of multi-modal follow-up data in a life-science study of the ageing processes in the brain, with a specific focus on multi-contrast MR rat brain data</li></ol>},
}

@phdthesis{Staring:2008,
  abbr =    {},
  bibtex_show = {true},
  author =    {Staring, M.},
  title =     {Intrasubject Registration for Change Analysis in Medical Imaging},
  school =    {Utrecht University},
  address =   {Image Sciences Institute},
  month =     {October},
  year =      {2008},
  pdf =       {2008_t_PhD.pdf},
  html =      {https://dspace.library.uu.nl/handle/1874/30513},
  abstract =  {Chapter 2 gives a general overview of image registration and describes the registration framework and software used in this thesis. The registration software was developed at the Image Sciences Institute jointly with Stefan Klein for our research projects. All experiments described in this thesis were performed with this software package, called <a href="http://lkeb-elas01.lumcnet.prod.intern">elastix</a>.<br>A first approach to tackle the rigid-nonrigid tissue problem uses adaptive filtering of the deformation field. The technique is reported in Chapter 3. Issues that remain with this approach are solved with another technique based on a local rigidity penalty term, see Chapter 4. The last technique is clinically employed for the detection of subtle changes in ground-glass opacities in the lung. The results of this study are presented in Chapter 5.<br>To address the problem of insufficient registration quality, in Chapter 6 the registration cost function is extended with multiple image features, instead of using image intensity only, as is commonly done.},
}

@mastersthesis{Staring:2002,
  abbr =    {},
  bibtex_show = {true},
  author =    {Staring, M.},
  title =     {Analysis of Quantization based Watermarking},
  school =    {University of Twente, Faculty of Applied Mathematics},
  address =   {Drienerlolaan 5, 7522 NB Enschede},
  month =     {December},
  year =      {2002},
  pdf =       {2002_t_QIM.pdf},
  html =      {},
  abstract =  {In order to protect (copy)rights of digital content, means are sought to stop piracy. Several methods are known to be instrumental for achieving this goal. This report considers one such method: digital watermarking, more specific quantization based watermarking methods. A general watermarking scheme consist of a watermark embedder, a channel representing some sort of processing on the watermarked signal, and a watermark detector. The problems related to any watermarking method are the perceptual quality of the watermarked signal, and the possibility to retrieve the embedded information at the detector.<br>From current quantization based watermarking algorithms, like QIM, DC-QIM, SCS, etc., it is known that the achievable rates are promising, but that it is hard to meet the required robustness demands. Therefore improvements of current algorithms are sought that are more robust against normal processing. This report focusses on two possible improvements, namely the use of error correcting codes (ECC) and the use of adaptive quantization.<br>Watermarking can be seen as a form of communication. Therefore, the robustness demand for watermarking is equivalent with the demand of reliable communication for communication models. Therefore, the use of ECC gives certainly an improvement in robustness. This is confirmed by experiments. Repetition codes are simple to implement and already gives a gain in robustness. The concatenation of convolutional codes with repetition codes gives an improvement only in the case of mild degradations due to the above mentioned processing.<br>In this report watermarking of signals with a luminance component are considered, like digital images and video data. Adaptive quantization refers to the use of a larger quantization step size for high luminance values, and a lower quantization step size for low luminance values. It is known from Weber's law that the human eye is less sensitive for brightness changes in higher luminance values, than it is in lower luminance values. Therefore, using adaptive quantization does not come at the cost of a loss of perceptual quality of the host signal. Adaptive quantization gives a large robustness gain for brightness scaling attacks. However, the adaptive quantization step size must be estimated at the detector, which potentially introduces an additional source of errors in the retrieved message. By means of experiments it is shown that this is not such a big problem. Therefore, adaptive quantization improves the robustness of the watermarking scheme.<br>It is valuable to know the performance of the watermarking scheme with the two improvements. The used performance measure is the bit error probability. The total bit error probability is build up from two components: One estimates the bit error probability for the case of fixed quantization, with an Additive White Gaussian Noise (AWGN) or uniform noise attack; The other estimates the bit error probability for the case of adaptive quantization, without any attack. Models for these two bit error probabilities are developed.<br>At the embedder the distortion compensation parameter <i>&alpha;</i> has to be set. The optimal value for this parameter is derived for the case of a Gaussian host signal and an AWGN channel. The value of this optimal parameter <i>&alpha;*</i> is compared with an earlier result of Eggers and is shown to be identical. But whereas Eggers found a numerical function, which he numerically optimizes, our result leads to an analytical function, which can be optimized numerically.<br>So, we use two methods to improve robustness, namely the use of error correcting codes and an adaptive quantization step size. These two methods are shown to be improvements. Also an analytical model for the performance is derived, which can be used to verify analytically the robustness improvement.},
}


