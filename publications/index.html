<!doctype html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Marius Staring </title> <meta name="author" content="Marius Staring"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>" > <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mstaring.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Marius</span> Staring </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <p>My Google Scholar profile can be found <a href="http://scholar.google.nl/citations?user=pKFkfq4AAAAJ">here</a>.</p> <div class="publications"> <h2 class="bibliography">Journal Articles</h2> <ol class="bibliography"><li><div class="row"> <div id="Mody:2024" class="col-sm-10"> <span class="title">Large-scale dose evaluation of deep learning organ contours in head-and-neck radiotherapy by leveraging existing plans</span> <span class="author"> Prerak Mody,&nbsp;Merle Huiskes,&nbsp;Nicolas Plaza , and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Alice Onderwater, Rense Lamsma, Klaus Hildebrandt, Nienke Hoekstra, Eleftheria Astreinidou, Marius Staring, Frank Dankers' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </span> <span class="periodical"> <em>Physics and Imaging in Radiation Oncology</em>, Apr 2024 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1016/j.phro.2024.100572" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2024_j_PHIRO.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> <a href="https://github.com/prerakmody/dose-eval-via-existing-plan-parameters"><img src="/assets/img/github-mark.svg" height="20"/></a> </div> <div class="abstract hidden"> <p><b>Background and Purpose:</b> Retrospective dose evaluation for organ-at-risk auto-contours has previously used small cohorts due to additional manual effort required for treatment planning on auto-contours. We aimed to do this at large scale, by a) proposing and assessing an automated plan optimization workflow that used existing clinical plan parameters and b) using it for head-and-neck auto-contour dose evaluation.<br/><b>Materials and Methods:</b> Our automated workflow emulated our clinic’s treatment planning protocol and reused existing clinical plan optimization parameters. This workflow recreated the original clinical plan (P<sub>OG</sub>) with manual contours (P<sub>MC</sub>) and evaluated the dose effect (P<sub>OG</sub> - P<sub>MC</sub>) on 70 photon and 30 proton plans of head-and-neck patients. As a use-case, the same workflow (and parameters) created a plan using auto-contours (P<sub>AC</sub>) of eight head-and-neck organs-at-risk from a commercial tool and evaluated their dose effect (P<sub>MC</sub> - P<sub>AC</sub>).<br/><b>Results:</b> For plan recreation (P<sub>OG</sub> - P<sub>MC</sub>), our workflow had a median impact of 1.0% and 1.5% across dose metrics of auto-contours, for photon and proton respectively. Computer time of automated planning was 25% (photon) and 42% (proton) of manual planning time. For auto-contour evaluation (P<sub>MC</sub> - P<sub>AC</sub>), we noticed an impact of 2.0% and 2.6% for photon and proton radiotherapy. All evaluations had a median &Delta;NTCP (Normal Tissue Complication Probability) less than 0.3%.<br/><b>Conclusions:</b> The plan replication capability of our automated program provides a blueprint for other clinics to perform auto-contour dose evaluation with large patient cohorts. Finally, despite geometric differences, auto-contours had a minimal median dose impact, hence inspiring confidence in their utility and facilitating their clinical adoption.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Mody:2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Large-scale dose evaluation of deep learning organ contours in head-and-neck radiotherapy by leveraging existing plans}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mody, Prerak and Huiskes, Merle and Chaves de Plaza, Nicolas and Onderwater, Alice and Lamsma, Rense and Hildebrandt, Klaus and Hoekstra, Nienke and Astreinidou, Eleftheria and Staring, Marius and Dankers, Frank}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Physics and Imaging in Radiation Oncology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{30}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{100572}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Stoel:2024" class="col-sm-10"> <span class="title">Deep Learning in Rheumatologic Image Interpretation</span> <span class="author"> Berend C. Stoel,&nbsp;<em>Marius Staring</em>,&nbsp;Monique Reijnierse , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Annette H.M. Helm-van Mil' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </span> <span class="periodical"> <em>Nature Reviews Rheumatology</em>, Mar 2024 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1038/s41584-023-01074-5" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2024_j_NRR.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> </div> <div class="abstract hidden"> <p>Artificial intelligence techniques, specifically deep learning, have already affected daily life in a wide range of areas. Likewise, initial applications have been explored in rheumatology. Deep learning might not easily surpass the accuracy of classic techniques when performing classification or regression on low-dimensional numerical data. With images as input, however, deep learning has become so successful that it has already outperformed the majority of conventional image-processing techniques developed during the past 50 years. As with any new imaging technology, rheumatologists and radiologists need to consider adapting their arsenal of diagnostic, prognostic and monitoring tools, and even their clinical role and collaborations. This adaptation requires a basic understanding of the technical background of deep learning, to efficiently utilize its benefits but also to recognize its drawbacks and pitfalls, as blindly relying on deep learning might be at odds with its capabilities. To facilitate such an understanding, it is necessary to provide an overview of deep-learning techniques for automatic image analysis in detecting, quantifying, predicting and monitoring rheumatic diseases, and of currently published deep-learning applications in radiological imaging for rheumatology, with critical assessment of possible limitations, errors and confounders, and conceivable consequences for rheumatologists and radiologists in clinical practice.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Stoel:2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep Learning in Rheumatologic Image Interpretation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stoel, Berend C. and Staring, Marius and Reijnierse, Monique and van der Helm-van Mil, Annette H.M.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Reviews Rheumatology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{20}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{182 -- 195}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Chen:2024" class="col-sm-10"> <span class="title">CoNeS: Conditional neural fields with shift modulation for multi-sequence MRI translation</span> <span class="author"> Yunjie Chen,&nbsp;<em>Marius Staring</em>,&nbsp;Olaf M. Neve , and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Stephan R. Romeijn, Erik F. Hensen, Berit M. Verbist, Jelmer M. Wolterink, Qian Tao' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </span> <span class="periodical"> <em>The Journal of Machine Learning for Biomedical Imaging</em>, Mar 2024 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.03320" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.59275/j.melba.2024-d61g" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2024_j_MELBA.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> <a href="https://github.com/cyjdswx/CoNeS.git"><img src="/assets/img/github-mark.svg" height="20"/></a> </div> <div class="abstract hidden"> <p>Multi-sequence magnetic resonance imaging (MRI) has found wide applications in both modern clinical studies and deep learning research. However, in clinical practice, it frequently occurs that one or more of the MRI sequences are missing due to different image acquisition protocols or contrast agent contraindications of patients, limiting the utilization of deep learning models trained on multi-sequence data. One promising approach is to leverage generative models to synthesize the missing sequences, which can serve as a surrogate acquisition. State-of-the-art methods tackling this problem are based on convolutional neural networks (CNN) which usually suffer from spectral biases, resulting in poor reconstruction of high-frequency fine details. In this paper, we propose Conditional Neural fields with Shift modulation (CoNeS), a model that takes voxel coordinates as input and learns a representation of the target images for multi-sequence MRI translation. The proposed model uses a multi-layer perceptron (MLP) instead of a CNN as the decoder for pixel-to-pixel mapping. Hence, each target image is represented as a neural field that is conditioned on the source image via shift modulation with a learned latent code. Experiments on BraTS 2018 and an in-house clinical dataset of vestibular schwannoma patients showed that the proposed method outperformed state-of-the-art methods for multi-sequence MRI translation both visually and quantitatively. Moreover, we conducted spectral analysis, showing that CoNeS was able to overcome the spectral bias issue common in conventional CNN models. To further evaluate the usage of synthesized images in clinical downstream tasks, we tested a segmentation network using the synthesized images at inference. The results showed that CoNeS improved the segmentation performance when some MRI sequences were missing and outperformed other synthesis models. We concluded that neural fields are a promising technique for multi-sequence MRI translation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Chen:2024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Yunjie and Staring, Marius and Neve, Olaf M. and Romeijn, Stephan R. and Hensen, Erik F. and Verbist, Berit M. and Wolterink, Jelmer M. and Tao, Qian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CoNeS: Conditional neural fields with shift modulation for multi-sequence MRI translation}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Journal of Machine Learning for Biomedical Imaging}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{657 -- 685}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Chaves-de-Plaza:2024" class="col-sm-10"> <span class="title">Inclusion Depth for Contour Ensembles</span> <span class="author"> Nicolas Chaves-de-Plaza,&nbsp;Prerak P. Mody,&nbsp;<em>Marius Staring</em> , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'René; Egmond, Anna Vilanova, Klaus Hildebrandt' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </span> <span class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Mar 2024 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1109/TVCG.2024.3350076" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2024_j_TVCG.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> <a href=""><img src="/assets/img/github-mark.svg" height="20"/></a> </div> <div class="abstract hidden"> <p>Ensembles of contours arise in various applications like simulation, computer-aided design, and semantic segmentation. Uncovering ensemble patterns and analyzing individual members is a challenging task that suffers from clutter. Ensemble statistical summarization can alleviate this issue by permitting analyzing ensembles’ distributional components like the mean and median, confidence intervals, and outliers. Contour boxplots, powered by Contour Band Depth (CBD), are a popular nonparametric ensemble summarization method that benefits from CBD’s generality, robustness, and theoretical properties. In this work, we introduce Inclusion Depth (ID), a new notion of contour depth with three defining characteristics. First, ID is a generalization of functional Half-Region Depth, which offers several theoretical guarantees. Second, ID relies on a simple principle: the inside/outside relationships between contours. This facilitates implementing ID and understanding its results. Third, the computational complexity of ID scales quadratically in the number of members of the ensemble, improving CBD’s cubic complexity. This also in practice speeds up the computation enabling the use of ID for exploring large contour ensembles or in contexts requiring multiple depth evaluations like clustering. In a series of experiments on synthetic data and case studies with meteorological and segmentation data, we evaluate ID’s performance and demonstrate its capabilities for the visual analysis of contour ensembles.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Chaves-de-Plaza:2024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chaves-de-Plaza, Nicolas and Mody, Prerak P. and Staring, Marius and van Egmond, Ren{\'e}; and Vilanova, Anna and Hildebrandt, Klaus}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Inclusion Depth for Contour Ensembles}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Visualization and Computer Graphics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Beljaards:2024" class="col-sm-10"> <span class="title">AI-Based Motion Artifact Severity Estimation in Undersampled MRI Allowing for Selection of Appropriate Reconstruction Models</span> <span class="author"> Laurens Beljaards,&nbsp;Nicola Pezzotti,&nbsp;Chinmay Rao , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Mariya Doneva, Matthias J.P. Osch, Marius Staring' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </span> <span class="periodical"> <em>Medical Physics</em>, Mar 2024 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1002/mp.16918" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2024_j_MP.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> <a href=""><img src="/assets/img/github-mark.svg" height="20"/></a> </div> <div class="abstract hidden"> <p><b>Background:</b> MR acquisition is a time consuming process, making it susceptible to patient motion during scanning. Even motion in the order of a millimeter can introduce severe blurring and ghosting artifacts, potentially necessitating re-acquisition. MRI can be accelerated by acquiring only a fraction of k-space, combined with advanced reconstruction techniques leveraging coil sensitivity profiles and prior knowledge. AI-based reconstruction techniques have recently been popularized, but generally assume an ideal setting without intra-scan motion.<br/><b>Purpose:</b> To retrospectively detect and quantify the severity of motion artifacts in undersampled MRI data. This may prove valuable as a safety mechanism for AI-based approaches, provide useful information to the reconstruction method, or prompt for re-acquisition while the patient is still in the scanner.<br/><b>Methods:</b> We developed a deep learning approach that detects and quantifies motion artifacts in undersampled brain MRI. We demonstrate that synthetically motion-corrupted data can be leveraged to train the CNN-based motion artifact estimator, generalizing well to real-world data. Additionally, we leverage the motion artifact estimator by using it as a selector for a motion-robust reconstruction model in case a considerable amount of motion was detected, and a high data consistency model otherwise.<br/><b>Results:</b> Training and validation were performed on 4387 and 1304 synthetically motion-corrupted images and their uncorrupted counterparts, respectively. Testing was performed on undersampled in vivo motion-corrupted data from 28 volunteers, where our model distinguished head motion from motion-free scans with 91% and 96% accuracy when trained on synthetic and on real data, respectively. It predicted a manually defined quality label (‘Good’, ‘Medium’ or ‘Bad’ quality) correctly in 76% and 85% of the time when trained on synthetic and real data, respectively. When used as a selector it selected the appropriate reconstruction network 93% of the time, achieving near optimal SSIM values.<br/><b>Conclusions:</b> The proposed method quantified motion artifact severity in undersampled MRI data with high accuracy, enabling real-time motion artifact detection that can help improve the safety and quality of AI-based reconstructions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Beljaards:2024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Beljaards, Laurens and Pezzotti, Nicola and Rao, Chinmay and Doneva, Mariya and van Osch, Matthias J.P. and Staring, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AI-Based Motion Artifact Severity Estimation in Undersampled MRI Allowing for Selection of Appropriate Reconstruction Models}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Medical Physics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{51}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3555 -- 3565}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Jia:2023" class="col-sm-10"> <span class="title">Automatic pulmonary function estimation from chest CT scans using deep regression neural networks: the relation between structure and function in systemic sclerosis</span> <span class="author"> Jingnan Jia,&nbsp;Emiel R. Marges,&nbsp;Maarten K. Ninaber , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Lucia J.M. Kroft, Anne A. Schouffoer, Marius Staring, Berend C. Stoel' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </span> <span class="periodical"> <em>IEEE Access</em>, Nov 2023 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1109/ACCESS.2023.3337639" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2023_j_Access.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> <a href=""><img src="/assets/img/github-mark.svg" height="20"/></a> </div> <div class="abstract hidden"> <p>Pulmonary function test (PFT) plays an important role in screening and following-up pulmonary involvement in systemic sclerosis (SSc). However, some patients are not able to perform PFT due to contraindications. In addition, it is unclear how lung function is affected by changes in lung structure in SSc. Therefore, this study aims to explore the potential of automatically estimating PFT results from chest CT scans of SSc patients and how different regions influence the estimation of PFT values. Deep regression networks were developed with transfer learning to estimate PFT from 316 SSc patients. Segmented lungs and vessels were used to mask the CT images to train the network with different inputs: from entire CT scan, lungs-only to vessels-only. The network trained by entire CT scans with transfer learning achieved an ICC of 0.71, 0.76, 0.80, and 0.81 for the estimation of DLCO, FEV1, FVC and TLC, respectively. The performance of the networks gradually decreased when trained on data from lungs-only and vessels-only. Regression attention maps showed that regions close to large vessels are highlighted more than other regions, and occasionally regions outside the lungs are highlighted. These experiments mean that apart from lungs and large vessels, other regions contribute to the estimation of PFTs. In addition, adding manually designed biomarkers increased the correlation (R) from 0.75, 0.74, 0.82, and 0.83 to 0.81, 0.83, 0.88, and 0.90, respectively. It means that that manually designed imaging biomarkers can still contribute to explaining the relation between lung function and structure.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Jia:2023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jia, Jingnan and Marges, Emiel R. and Ninaber, Maarten K. and Kroft, Lucia J.M. and Schouffoer, Anne A. and Staring, Marius and Stoel, Berend C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic pulmonary function estimation from chest CT scans using deep regression neural networks: the relation between structure and function in systemic sclerosis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Access}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{135272 -- 135282}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Neve:2023" class="col-sm-10"> <span class="title">Automated 2-dimensional measurement of vestibular schwannoma: validity and accuracy of an artificial intelligence algorithm</span> <span class="author"> Olaf M. Neve,&nbsp;Stephan R. Romeijn,&nbsp;Yunjie Chen , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Larissa Nagtegaal, Willem Grootjans, Jeroen C. Jansen, Marius Staring, Berit M. Verbist, Erik F. Hensen' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </span> <span class="periodical"> <em>Otolaryngology - Head and Neck Surgery</em>, Dec 2023 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1002/ohn.470" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2023_j_OHNS.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> <a href=""><img src="/assets/img/github-mark.svg" height="20"/></a> </div> <div class="abstract hidden"> <p><b>Objective.</b> Validation of automated 2-dimensional (2D) diameter measurements of vestibular schwannomas on magnetic resonance imaging (MRI).<br/><b>Study Design.</b>Retrospective validation study using 2 data sets containing MRIs of vestibular schwannoma patients.<br/><b>Setting.</b> University Hospital in The Netherlands.<br/><b>Methods.</b>Two data sets were used, 1 containing 1 scan per patient (n = 134) and the other containing at least 3 consecutive MRIs of 51 patients, all with contrast-enhanced T1 or high-resolution T2 sequences. 2D measurements of the maximal extrameatal diameters in the axial plane were automatically derived from a 3D-convolutional neural network compared to manual measurements by 2 human observers. Intra- and interobserver variabilities were calculated using the intraclass correlation coefficient (ICC), agreement on tumor progression using Cohen’s kappa.<br/><b>Results.</b> The human intra- and interobserver variability showed a high correlation (ICC: 0.98-0.99) and limits of agreement of 1.7 to 2.1 mm. Comparing the automated to human measurements resulted in ICC of 0.98 (95% confidence interval [CI]: 0.974; 0.987) and 0.97 (95% CI: 0.968; 0.984), with limits of agreement of 2.2 and 2.1 mm for diameters parallel and perpendicular to the posterior side of the temporal bone, respectively. There was satisfactory agreement on tumor progression between automated measurements and human observers (Cohen’s &kappa; = 0.77), better than the agreement between the human observers (Cohen’s &kappa; = 0.74).<br/><b>Conclusion.</b> Automated 2D diameter measurements and growth detection of vestibular schwannomas are at least as accurate as human 2D measurements. In clinical practice, measurements of the maximal extrameatal tumor (2D) diameters of vestibular schwannomas provide important complementary information to total tumor volume (3D) measurements. Combining both in an automated measurement algorithm facilitates clinical adoption.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Neve:2023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Neve, Olaf M. and Romeijn, Stephan R. and Chen, Yunjie and Nagtegaal, Larissa and Grootjans, Willem and Jansen, Jeroen C. and Staring, Marius and Verbist, Berit M. and Hensen, Erik F.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated 2-dimensional measurement of vestibular schwannoma: validity and accuracy of an artificial intelligence algorithm}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Otolaryngology - Head and Neck Surgery}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{169}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1582 -- 1589}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Zhai:2023" class="col-sm-10"> <span class="title">Automated Quantification of the Pulmonary Vasculature in Pulmonary Embolism and Chronic Thromboembolic Pulmonary Hypertension</span> <span class="author"> Zhiwei Zhai,&nbsp;Gudula J.A.M. Boon,&nbsp;<em>Marius Staring</em> , and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Lisette F. Dam, Lucia J.M. Kroft, Irene Hernandez Giron, Maarten K. Ninaber, Harm Jan Bogaard, Lilian J. Meijboom, Anton Vonk Noordegraaf, Menno V. Huisman, Frederikus A. Klok, Berend C. Stoel' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </span> <span class="periodical"> <em>Pulmonary Circulation</em>, Dec 2023 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1002/pul2.12223" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2023_j_PC.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> <a href=""><img src="/assets/img/github-mark.svg" height="20"/></a> </div> <div class="abstract hidden"> <p>The particular mechanical obstruction of pulmonary embolism (PE) and chronic thromboembolic pulmonary hypertension (CTEPH) may affect pulmonary arteries and veins differently. Therefore, we evaluated whether pulmonary vascular morphology and densitometry using CT pulmonary angiography (CTPA) in arteries and veins could distinguish PE from CTEPH.<br/>We analyzed CTPA images from a convenience cohort of 16 PE patients, 6 CTEPH patients and 15 controls without PE or CTEPH. Pulmonary vessels were extracted with a graph-cuts method, and separated into arteries and veins using a deep-learning classification method. By analyzing the distribution of vessel radii, vascular morphology was quantified into a slope (&alpha;) and intercept (&beta;) for the entire pulmonary vascular tree, and for arteries and veins, separately. To quantify lung perfusion, the median pulmonary vascular density was calculated. As a reference, lung perfusion was also quantified by the contrast enhancement in the parenchymal areas, pulmonary trunk and descending aorta. All quantifications were compared between the three groups.<br/>Vascular morphology did not differ between groups, in contrast to vascular density values (both arterial and venous; p-values 0.006 - 0.014). The median vascular density (interquartile range) was -452 (95), -567 (113) and -470 (323) HU, for the PE, control and CTEPH group, respectively. The perfusion curves from all measurements showed different patterns between groups.<br/>In this proof of concept study, not vasculature morphology but vascular densities differentiated between normal and thrombotic obstructed vasculature. For distinction on an individual patient level, further technical improvements are needed both in terms of image acquisition/reconstruction and post-processing.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Zhai:2023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhai, Zhiwei and Boon, Gudula J.A.M. and Staring, Marius and van Dam, Lisette F. and Kroft, Lucia J.M. and Giron, Irene Hernandez and Ninaber, Maarten K. and Bogaard, Harm Jan and Meijboom, Lilian J. and Vonk Noordegraaf, Anton and Huisman, Menno V. and Klok, Frederikus A. and Stoel, Berend C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated Quantification of the Pulmonary Vasculature in Pulmonary Embolism and Chronic Thromboembolic Pulmonary Hypertension}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Pulmonary Circulation}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{e12223}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Goedmakers:2022" class="col-sm-10"> <span class="title">Machine learning for image analysis in the cervical spine: Systematic review of the available models and methods</span> <span class="author"> C.M.W Goedmakers,&nbsp;L.M. Pereboom,&nbsp;J.W. Schoones , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'M.L. Bouter, R.F. Remis, M. Staring, C.L.A. Vleggeert-Lankamp' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </span> <span class="periodical"> <em>Brain and Spine</em>, Dec 2022 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1016/j.bas.2022.101666" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2022_j_BandS.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> <a href=""><img src="/assets/img/github-mark.svg" height="20"/></a> </div> <div class="abstract hidden"> <p><ul><li>Neural network approaches show the most potential for automated image analysis of thecervical spine.</li><li>Fully automatic convolutional neural network (CNN) models are promising Deep Learning methods for segmentation.</li><li>In cervical spine analysis, the biomechanical features are most often studied using finiteelement models.</li><li>The application of artificial neural networks and support vector machine models looks promising for classification purposes.</li><li>This article provides an overview of the methods for research on computer aided imaging diagnostics of the cervical spine.</li></ul></p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Goedmakers:2022</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Goedmakers, C.M.W and Pereboom, L.M. and Schoones, J.W. and de Leeuw den Bouter, M.L. and Remis, R.F. and Staring, M. and Vleggeert-Lankamp, C.L.A.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Machine learning for image analysis in the cervical spine: Systematic review of the available models and methods}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Brain and Spine}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{101666}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Neve:2022" class="col-sm-10"> <span class="title">Fully Automated 3D Vestibular Schwannoma Segmentation with and without Gadolinium Contrast: a multi-center, multi-vendor study</span> <span class="author"> Olaf Neve,&nbsp;Yunjie Chen,&nbsp;Qian Tao , and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Stephan Romeijn, Nick Boer, Willem Grootjans, Mark Kruit, Boudewijn Lelieveldt, Jeroen Jansen, Erik Hensen, Berit Verbist, Marius Staring' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </span> <span class="periodical"> <em>Radiology: Artificial Intelligence</em>, Dec 2022 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1148/ryai.210300" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2022_j_RadAI.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> </div> <div class="abstract hidden"> <p><b>Purpose:</b> To develop automated vestibular schwannoma measurements on contrast-enhanced T1- and T2-weighted MRI.<br/><b>Material and methods:</b> MRI data from 214 patients in 37 different centers was retrospectively analyzed between 2020-2021. Patients with hearing loss (134 vestibular schwannoma positive [mean age ± SD, 54 ± 12 years; 64 men], 80 negative) were randomized to a training and validation set and an independent test set. A convolutional neural network (CNN) was trained using five-fold cross-validation for two models (T1 and T2). Quantitative analysis including Dice index, Hausdorff distance, surface-to-surface distance (S2S), and relative volume error were used to compare the computer and the human delineations. Furthermore, an observer study was performed in which two experienced physicians evaluated both delineations.<br/><b>Results:</b> The T1-weighted model showed state-of-the-art performance with a mean S2S distance of less than 0.6 mm for the whole tumor and the intrameatal and extrameatal tumor parts. The whole tumor Dice index and Hausdorff distance were 0.92 and 2.1 mm in the independent test set. T2-weighted images had a mean S2S distance less than 0.6 mm for the whole tumor and the intrameatal and extrameatal tumor parts. Whole tumor Dice index and Hausdorff distance were 0.87 and 1.5 mm in the independent test set. The observer study indicated that the tool was comparable to human delineations in 85-92% of cases.<br/><b>Conclusion:</b> The CNN model detected and delineated vestibular schwannomas accurately on contrast-enhanced T1 and T2-weighted MRI and distinguished the clinically relevant difference between intrameatal and extrameatal tumor parts.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Neve:2022</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Neve, Olaf and Chen, Yunjie and Tao, Qian and Romeijn, Stephan and de Boer, Nick and Grootjans, Willem and Kruit, Mark and Lelieveldt, Boudewijn and Jansen, Jeroen and Hensen, Erik and Verbist, Berit and Staring, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fully Automated 3D Vestibular Schwannoma Segmentation with and without Gadolinium Contrast: a multi-center, multi-vendor study}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Radiology: Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{e210300}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Koolstra:2022" class="col-sm-10"> <span class="title">Subject-specific optimization of background suppression for arterial spin labeling MRI using a feedback loop on the scanner</span> <span class="author"> Kirsten Koolstra,&nbsp;<em>Marius Staring</em>,&nbsp;Paul Bruin , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Mathias J.P. Osch' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </span> <span class="periodical"> <em>NMR in Biomedicine</em>, Sep 2022 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1002/nbm.4746" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2022_j_NMR.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> <a href=""><img src="/assets/img/github-mark.svg" height="20"/></a> </div> <div class="abstract hidden"> <p>Background suppression (BGS) in arterial spin labeling (ASL) MRI leads to a higher temporal SNR (tSNR) of the perfusion images compared to ASL without BGS. The performance of the BGS, however, depends on the tissue relaxation times and on inhomogeneities of the scanner’s magnetic fields, which differ between subjects and are unknown at the moment of scanning. Therefore, we developed a feedback loop (FBL) mechanism that optimizes the BGS for each subject in the scanner during acquisition. We implemented the FBL for 2D pseudo-continuous ASL (PCASL) scans with an echo-planar imaging (EPI) readout. After each dynamic scan, acquired ASL images were automatically sent to an external computer and processed with a Python processing tool. Inversion times were optimized on-the-fly using 80 iterations of the Nelder-Mead method, by minimizing the signal intensity in the label image while maximizing the signal intensity in the perfusion image. The performance of this method was first tested in a 4-component phantom. The regularization parameter was then tuned in 6 healthy subjects (3 male, 3 female, age 24-62 years) and set as &lambda;=4 for all other experiments. Resulting ASL images, perfusion images and tSNR maps obtained from the last 20 iterations of the FBL scan were compared to those obtained without BGS and to standard BGS in 12 healthy volunteers (5 male, 7 female, age 24-62 years) (including the 6 volunteers used for tuning of &lambda;). The FBL resulted in perfusion images with a statistically significantly higher tSNR (2.20) compared to standard BGS (1.96) (P &lt; 5 10<sup>-3</sup>, two-sided paired t-test). Minimizing signal in the label image furthermore resulted in control images from which approximate changes in perfusion signal can directly be appreciated. This could be relevant to ASL applications that require a high temporal resolution. Future work is needed to minimize the number of initial acquisitions during which the performance of BGS is reduced compared to standard BGS and to extend the technique to 3D ASL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Koolstra:2022</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Koolstra, Kirsten and Staring, Marius and de Bruin, Paul and van Osch, Mathias J.P.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Subject-specific optimization of background suppression for arterial spin labeling MRI using a feedback loop on the scanner}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NMR in Biomedicine}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{e4746}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Brink:2022" class="col-sm-10"> <span class="title">Personalised Local SAR Prediction for Parallel Transmit Neuroimaging at 7T from a Single T1-weighted Dataset</span> <span class="author"> Wyger M. Brink,&nbsp;Sahar Yousefi,&nbsp;Prerna Bhatnagar , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Rob F. Remis, Marius Staring, Andrew G. Webb' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </span> <span class="periodical"> <em>Magnetic Resonance in Medicine</em>, Jul 2022 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1002/mrm.29215" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2022_j_MRM.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> <a href="https://github.com/wygerbrink/PersonalizedDosimetry"><img src="/assets/img/github-mark.svg" height="20"/></a> </div> <div class="abstract hidden"> <p><b>Purpose.</b> Parallel RF transmission (PTx) is one of the key technologies enabling high quality imaging at ultrahigh field strengths (&ge;7T). Compliance with regulatory limits on the local specific absorption rate (SAR) typically involves over-conservative safety margins to account for intersubject variability, which negatively affect the utilization of ultra-high field MR. In this work, we present a method to generate a subject-specific body model from a single T1-weighted dataset for personalized local SAR prediction in PTx neuroimaging at 7T.<br/><b>Methods.</b> Multi-contrast data were acquired at 7T (N=10) to establish ground truth segmentations in eight tissue types. A 2.5D convolutional neural network was trained using the T1-weighted data as input in a leave-one-out cross-validation study. The segmentation accuracy was evaluated through local SAR simulations in a quadrature birdcage as well as a PTx coil model.<br/><b>Results.</b> The network-generated segmentations reached overall Dice coefficients of 86.7% &plusmn; 6.7% (mean &plusmn; standard deviation) and showed to successfully address the severe intensity bias and contrast variations typical to 7T. Errors in peak local SAR obtained were below 3.0% in the quadrature birdcage. Results obtained in the PTx configuration indicated that a safety margin of 6.3% ensures conservative local SAR estimates in 95% of the random RF shims, compared to an average overestimation of 34% in the generic "one-size-fits-all" approach.<br/><b>Conclusion.</b> A subject-specific body model can be automatically generated from a single T1-weighted dataset by means of deep learning, providing the necessary inputs for accurate and personalized local SAR predictions in PTx neuroimaging at 7T.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Brink:2022</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brink, Wyger M. and Yousefi, Sahar and Bhatnagar, Prerna and Remis, Rob F. and Staring, Marius and Webb, Andrew G.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Personalised Local SAR Prediction for Parallel Transmit Neuroimaging at 7T from a Single T1-weighted Dataset}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Magnetic Resonance in Medicine}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{88}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{464 - 475}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li><div class="row"> <div id="Malimban:2022" class="col-sm-10"> <span class="title">Deep learning-based segmentation of the thorax in mouse micro-CT scans</span> <span class="author"> Justin Malimban,&nbsp;Danny Lathouwers,&nbsp;Haibin Qian , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Frank Verhaegen, Julia Wiedemann, Sytze Brandenburg, Marius Staring' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </span> <span class="periodical"> <em>Scientific Reports</em>, Jul 2022 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1038/s41598-022-05868-7" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2022_j_SR.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> <a href=""><img src="/assets/img/github-mark.svg" height="20"/></a> </div> <div class="abstract hidden"> <p>For image-guided small animal irradiations, the whole workflow of imaging, organ contouring, irradiation planning, and delivery is typically performed in a single session requiring continuous administration of anesthetic agents. Automating contouring leads to a faster workflow, which limits exposure to anesthesia and thereby, reducing its impact on experimental results and on animal wellbeing. Here, we trained the 2D and 3D U-Net architectures of no-new-Net (nnU-Net) for autocontouring of the thorax in mouse micro-CT images. We trained the models only on native CTs and evaluated their performance using an independent testing dataset (i.e., native CTs not included in the training and validation). Unlike previous studies, we also tested the model performance on an external dataset (i.e., contrast-enhanced CTs) to see how well they predict on CTs completely different from what they were trained on. We also assessed the interobserver variability using the generalized conformity index (CIgen) among three observers, providing a stronger human baseline for evaluating automated contours than previous studies. Lastly, we showed the benefit on the contouring time compared to manual contouring. The results show that 3D models of nnU-Net achieve superior segmentation accuracy and are more robust to unseen data than 2D models. For all target organs, the mean surface distance (MSD) and the Hausdorff distance (95p HD) of the best performing model for this task (nnU-Net 3d_fullres) are within 0.16 mm and 0.60 mm, respectively. These values are below the minimum required contouring accuracy of 1 mm for small animal irradiations, and improve significantly upon state-of-the-art 2D U-Net-based AIMOS method. Moreover, the conformity indices of the 3d_fullres model also compare favourably to the interobserver variability for all target organs, whereas the 2D models perform poorly in this regard. Importantly, the 3d_fullres model offers 98% reduction in contouring time.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Malimban:2022</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Malimban, Justin and Lathouwers, Danny and Qian, Haibin and Verhaegen, Frank and Wiedemann, Julia and Brandenburg, Sytze and Staring, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep learning-based segmentation of the thorax in mouse micro-CT scans}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Scientific Reports}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1822}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">Conference Articles</h2> <ol class="bibliography"><li><div class="row"> <div id="vanderValk:2023" class="col-sm-10"> <span class="title">Joint optimization of a β-VAE for ECG task-specific feature extraction</span> <span class="author"> Viktor Valk,&nbsp;Douwe Atsma,&nbsp;Roderick Scherptong , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Marius Staring' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </span> <span class="periodical"> <em>In Medical Image Computing and Computer-Assisted Intervention</em> , Oct 2023 </span> <span class="periodical"> </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2304.06476" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1007/978-3-031-43895-0_52" class="btn btn-sm z-depth-0" role="button">DOI</a> <a href="/assets/pdf/2023_c_MICCAI.pdf"><img src="/assets/img/PDF_32.png" height="20"/></a> </div> <div class="abstract hidden"> <p>Electrocardiography is the most common method to investigate the condition of the heart through the observation of cardiac rhythm and electrical activity, for both diagnosis and monitoring purposes. Analysis of electrocardiograms (ECGs) is commonly performed through the investigation of specific patterns, which are visually recognizable by trained physicians and are known to reflect cardiac (dis)function. In this work we study the use of β-variational autoencoders (VAEs) as an explainable feature extractor, and improve on its predictive capacities by jointly optimizing signal reconstruction and cardiac function prediction. The extracted features are then used for cardiac function prediction using logistic regression. The method is trained and tested on data from 7255 patients, who were treated for acute coronary syndrome at the Leiden University Medical Center between 2010 and 2021. The results show that our method significantly improved prediction and explainability compared to a vanilla β-VAE, while still yielding similar reconstruction performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vanderValk:2023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{van der Valk, Viktor and Atsma, Douwe and Scherptong, Roderick and Staring, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Joint optimization of a β-VAE for ECG task-specific feature extraction}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Medical Image Computing and Computer-Assisted Intervention}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Lecture Notes in Computer Science}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{14221}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{554 -- 563}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> &copy; Copyright 2024 Marius Staring. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with the <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Last updated: July 09, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hideBreadcrumbs noAutoLoadMdIcons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"news-we-received-funding-from-the-wellcome-trust-as-part-of-the-czi-essential-open-source-software-for-science-program-for-the-further-development-of-our-image-registration-software-lt-a-href-quot-https-elastix-dev-quot-gt-elastix-lt-a-gt",title:"We received funding from the Wellcome Trust as part of the \u201cCZI Essential Open Source Software for Science\u201d program for the further development of our image registration software &lt;a href=&quot;https://elastix.dev/&quot;&gt;elastix&lt;/a&gt;.",description:"",section:"News"},{id:"news-i-held-my-inaugural-lecture-entitled-images-and-physicians-in-transition-the-era-of-the-learning-machine",title:"I held my inaugural lecture entitled \u201cImages and Physicians in transition: the era of the learning machine\u201d.",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D.%73%74%61%72%69%6E%67@%6C%75%6D%63.%6E%6C","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0003-2885-5812","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=pKFkfq4AAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/2602873","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Marius-Staring-47869146/","_blank")}},{id:"socials-ieee-xplore",title:"IEEE Xplore",section:"Socials",handler:()=>{window.open("https://ieeexplore.ieee.org/author/38294698000/","_blank")}},{id:"socials-scopus",title:"Scopus",section:"Socials",handler:()=>{window.open("https://www.scopus.com/authid/detail.uri?authorId=55886447500","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/mstaring","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/mariusstaring","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/35/6426.html","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>